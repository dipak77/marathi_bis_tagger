{
  "best_metric": 0.20474353432655334,
  "best_model_checkpoint": "marathi_pos_model_bis2\\checkpoint-3636",
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 6060,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00825082508250825,
      "grad_norm": 4.29246711730957,
      "learning_rate": 4.991749174917492e-05,
      "loss": 2.4263,
      "step": 10
    },
    {
      "epoch": 0.0165016501650165,
      "grad_norm": 7.746501922607422,
      "learning_rate": 4.9834983498349835e-05,
      "loss": 1.492,
      "step": 20
    },
    {
      "epoch": 0.024752475247524754,
      "grad_norm": 10.91235637664795,
      "learning_rate": 4.975247524752475e-05,
      "loss": 1.1176,
      "step": 30
    },
    {
      "epoch": 0.033003300330033,
      "grad_norm": 4.935039043426514,
      "learning_rate": 4.9669966996699675e-05,
      "loss": 0.9891,
      "step": 40
    },
    {
      "epoch": 0.041254125412541254,
      "grad_norm": 4.706806182861328,
      "learning_rate": 4.9587458745874585e-05,
      "loss": 0.7421,
      "step": 50
    },
    {
      "epoch": 0.04950495049504951,
      "grad_norm": 5.4098992347717285,
      "learning_rate": 4.950495049504951e-05,
      "loss": 0.8036,
      "step": 60
    },
    {
      "epoch": 0.057755775577557754,
      "grad_norm": 5.126989364624023,
      "learning_rate": 4.9422442244224425e-05,
      "loss": 0.6684,
      "step": 70
    },
    {
      "epoch": 0.066006600660066,
      "grad_norm": 7.709214687347412,
      "learning_rate": 4.933993399339934e-05,
      "loss": 0.5824,
      "step": 80
    },
    {
      "epoch": 0.07425742574257425,
      "grad_norm": 5.320013999938965,
      "learning_rate": 4.925742574257426e-05,
      "loss": 0.5871,
      "step": 90
    },
    {
      "epoch": 0.08250825082508251,
      "grad_norm": 5.322455883026123,
      "learning_rate": 4.917491749174918e-05,
      "loss": 0.5727,
      "step": 100
    },
    {
      "epoch": 0.09075907590759076,
      "grad_norm": 4.665865898132324,
      "learning_rate": 4.909240924092409e-05,
      "loss": 0.5404,
      "step": 110
    },
    {
      "epoch": 0.09900990099009901,
      "grad_norm": 5.169902801513672,
      "learning_rate": 4.9009900990099014e-05,
      "loss": 0.5629,
      "step": 120
    },
    {
      "epoch": 0.10726072607260725,
      "grad_norm": 4.778602600097656,
      "learning_rate": 4.892739273927393e-05,
      "loss": 0.5281,
      "step": 130
    },
    {
      "epoch": 0.11551155115511551,
      "grad_norm": 3.6903090476989746,
      "learning_rate": 4.884488448844885e-05,
      "loss": 0.4858,
      "step": 140
    },
    {
      "epoch": 0.12376237623762376,
      "grad_norm": 2.8644886016845703,
      "learning_rate": 4.8762376237623764e-05,
      "loss": 0.4354,
      "step": 150
    },
    {
      "epoch": 0.132013201320132,
      "grad_norm": 4.598601818084717,
      "learning_rate": 4.867986798679868e-05,
      "loss": 0.4848,
      "step": 160
    },
    {
      "epoch": 0.14026402640264027,
      "grad_norm": 4.770663738250732,
      "learning_rate": 4.8597359735973603e-05,
      "loss": 0.4882,
      "step": 170
    },
    {
      "epoch": 0.1485148514851485,
      "grad_norm": 4.236464500427246,
      "learning_rate": 4.851485148514851e-05,
      "loss": 0.4301,
      "step": 180
    },
    {
      "epoch": 0.15676567656765678,
      "grad_norm": 5.021366119384766,
      "learning_rate": 4.8432343234323437e-05,
      "loss": 0.5216,
      "step": 190
    },
    {
      "epoch": 0.16501650165016502,
      "grad_norm": 5.556509017944336,
      "learning_rate": 4.834983498349835e-05,
      "loss": 0.4627,
      "step": 200
    },
    {
      "epoch": 0.17326732673267325,
      "grad_norm": 3.5752501487731934,
      "learning_rate": 4.826732673267327e-05,
      "loss": 0.3905,
      "step": 210
    },
    {
      "epoch": 0.18151815181518152,
      "grad_norm": 4.546638488769531,
      "learning_rate": 4.8184818481848186e-05,
      "loss": 0.4059,
      "step": 220
    },
    {
      "epoch": 0.18976897689768976,
      "grad_norm": 4.098177433013916,
      "learning_rate": 4.810231023102311e-05,
      "loss": 0.4177,
      "step": 230
    },
    {
      "epoch": 0.19801980198019803,
      "grad_norm": 4.376943588256836,
      "learning_rate": 4.801980198019802e-05,
      "loss": 0.4368,
      "step": 240
    },
    {
      "epoch": 0.20627062706270627,
      "grad_norm": 6.123010635375977,
      "learning_rate": 4.793729372937294e-05,
      "loss": 0.4155,
      "step": 250
    },
    {
      "epoch": 0.2145214521452145,
      "grad_norm": 3.7019317150115967,
      "learning_rate": 4.785478547854786e-05,
      "loss": 0.3767,
      "step": 260
    },
    {
      "epoch": 0.22277227722772278,
      "grad_norm": 4.234233379364014,
      "learning_rate": 4.7772277227722775e-05,
      "loss": 0.3467,
      "step": 270
    },
    {
      "epoch": 0.23102310231023102,
      "grad_norm": 2.6841061115264893,
      "learning_rate": 4.768976897689769e-05,
      "loss": 0.4181,
      "step": 280
    },
    {
      "epoch": 0.23927392739273928,
      "grad_norm": 3.3259315490722656,
      "learning_rate": 4.760726072607261e-05,
      "loss": 0.3907,
      "step": 290
    },
    {
      "epoch": 0.24752475247524752,
      "grad_norm": 3.9631388187408447,
      "learning_rate": 4.7524752475247525e-05,
      "loss": 0.3438,
      "step": 300
    },
    {
      "epoch": 0.25577557755775576,
      "grad_norm": 4.436331748962402,
      "learning_rate": 4.744224422442244e-05,
      "loss": 0.329,
      "step": 310
    },
    {
      "epoch": 0.264026402640264,
      "grad_norm": 3.926816701889038,
      "learning_rate": 4.7359735973597365e-05,
      "loss": 0.3358,
      "step": 320
    },
    {
      "epoch": 0.2722772277227723,
      "grad_norm": 4.5005950927734375,
      "learning_rate": 4.7277227722772274e-05,
      "loss": 0.3353,
      "step": 330
    },
    {
      "epoch": 0.28052805280528054,
      "grad_norm": 4.699954032897949,
      "learning_rate": 4.71947194719472e-05,
      "loss": 0.4137,
      "step": 340
    },
    {
      "epoch": 0.2887788778877888,
      "grad_norm": 3.0814549922943115,
      "learning_rate": 4.7112211221122114e-05,
      "loss": 0.3368,
      "step": 350
    },
    {
      "epoch": 0.297029702970297,
      "grad_norm": 5.954124450683594,
      "learning_rate": 4.702970297029703e-05,
      "loss": 0.3544,
      "step": 360
    },
    {
      "epoch": 0.30528052805280526,
      "grad_norm": 3.2172834873199463,
      "learning_rate": 4.694719471947195e-05,
      "loss": 0.3687,
      "step": 370
    },
    {
      "epoch": 0.31353135313531355,
      "grad_norm": 3.557965040206909,
      "learning_rate": 4.686468646864687e-05,
      "loss": 0.3267,
      "step": 380
    },
    {
      "epoch": 0.3217821782178218,
      "grad_norm": 3.5177783966064453,
      "learning_rate": 4.678217821782179e-05,
      "loss": 0.3897,
      "step": 390
    },
    {
      "epoch": 0.33003300330033003,
      "grad_norm": 3.356013774871826,
      "learning_rate": 4.6699669966996704e-05,
      "loss": 0.3228,
      "step": 400
    },
    {
      "epoch": 0.33828382838283827,
      "grad_norm": 3.06795072555542,
      "learning_rate": 4.661716171617162e-05,
      "loss": 0.3732,
      "step": 410
    },
    {
      "epoch": 0.3465346534653465,
      "grad_norm": 5.14241361618042,
      "learning_rate": 4.653465346534654e-05,
      "loss": 0.2851,
      "step": 420
    },
    {
      "epoch": 0.3547854785478548,
      "grad_norm": 3.391496181488037,
      "learning_rate": 4.645214521452145e-05,
      "loss": 0.3006,
      "step": 430
    },
    {
      "epoch": 0.36303630363036304,
      "grad_norm": 3.5572922229766846,
      "learning_rate": 4.636963696369637e-05,
      "loss": 0.3363,
      "step": 440
    },
    {
      "epoch": 0.3712871287128713,
      "grad_norm": 4.348113536834717,
      "learning_rate": 4.628712871287129e-05,
      "loss": 0.4121,
      "step": 450
    },
    {
      "epoch": 0.3795379537953795,
      "grad_norm": 3.4740560054779053,
      "learning_rate": 4.62046204620462e-05,
      "loss": 0.3354,
      "step": 460
    },
    {
      "epoch": 0.38778877887788776,
      "grad_norm": 2.760148048400879,
      "learning_rate": 4.6122112211221126e-05,
      "loss": 0.3239,
      "step": 470
    },
    {
      "epoch": 0.39603960396039606,
      "grad_norm": 4.156641483306885,
      "learning_rate": 4.603960396039604e-05,
      "loss": 0.3719,
      "step": 480
    },
    {
      "epoch": 0.4042904290429043,
      "grad_norm": 5.220545768737793,
      "learning_rate": 4.595709570957096e-05,
      "loss": 0.3257,
      "step": 490
    },
    {
      "epoch": 0.41254125412541254,
      "grad_norm": 4.409673690795898,
      "learning_rate": 4.5874587458745876e-05,
      "loss": 0.3237,
      "step": 500
    },
    {
      "epoch": 0.4207920792079208,
      "grad_norm": 3.974851131439209,
      "learning_rate": 4.57920792079208e-05,
      "loss": 0.3196,
      "step": 510
    },
    {
      "epoch": 0.429042904290429,
      "grad_norm": 4.440283298492432,
      "learning_rate": 4.570957095709571e-05,
      "loss": 0.4082,
      "step": 520
    },
    {
      "epoch": 0.4372937293729373,
      "grad_norm": 4.944815158843994,
      "learning_rate": 4.562706270627063e-05,
      "loss": 0.3448,
      "step": 530
    },
    {
      "epoch": 0.44554455445544555,
      "grad_norm": 4.006723880767822,
      "learning_rate": 4.554455445544555e-05,
      "loss": 0.315,
      "step": 540
    },
    {
      "epoch": 0.4537953795379538,
      "grad_norm": 4.259995460510254,
      "learning_rate": 4.5462046204620465e-05,
      "loss": 0.2762,
      "step": 550
    },
    {
      "epoch": 0.46204620462046203,
      "grad_norm": 4.726675510406494,
      "learning_rate": 4.537953795379538e-05,
      "loss": 0.348,
      "step": 560
    },
    {
      "epoch": 0.47029702970297027,
      "grad_norm": 5.088724136352539,
      "learning_rate": 4.52970297029703e-05,
      "loss": 0.281,
      "step": 570
    },
    {
      "epoch": 0.47854785478547857,
      "grad_norm": 3.4611785411834717,
      "learning_rate": 4.5214521452145214e-05,
      "loss": 0.3117,
      "step": 580
    },
    {
      "epoch": 0.4867986798679868,
      "grad_norm": 2.328251838684082,
      "learning_rate": 4.513201320132013e-05,
      "loss": 0.2974,
      "step": 590
    },
    {
      "epoch": 0.49504950495049505,
      "grad_norm": 3.505772829055786,
      "learning_rate": 4.5049504950495054e-05,
      "loss": 0.3387,
      "step": 600
    },
    {
      "epoch": 0.5033003300330033,
      "grad_norm": 3.650068759918213,
      "learning_rate": 4.4966996699669964e-05,
      "loss": 0.288,
      "step": 610
    },
    {
      "epoch": 0.5115511551155115,
      "grad_norm": 3.9693763256073,
      "learning_rate": 4.488448844884489e-05,
      "loss": 0.3464,
      "step": 620
    },
    {
      "epoch": 0.5198019801980198,
      "grad_norm": 3.9495480060577393,
      "learning_rate": 4.4801980198019804e-05,
      "loss": 0.2914,
      "step": 630
    },
    {
      "epoch": 0.528052805280528,
      "grad_norm": 4.256389617919922,
      "learning_rate": 4.471947194719473e-05,
      "loss": 0.3057,
      "step": 640
    },
    {
      "epoch": 0.5363036303630363,
      "grad_norm": 3.9943840503692627,
      "learning_rate": 4.463696369636964e-05,
      "loss": 0.296,
      "step": 650
    },
    {
      "epoch": 0.5445544554455446,
      "grad_norm": 4.543951988220215,
      "learning_rate": 4.455445544554456e-05,
      "loss": 0.3309,
      "step": 660
    },
    {
      "epoch": 0.5528052805280528,
      "grad_norm": 3.562497615814209,
      "learning_rate": 4.447194719471948e-05,
      "loss": 0.3217,
      "step": 670
    },
    {
      "epoch": 0.5610561056105611,
      "grad_norm": 3.517035484313965,
      "learning_rate": 4.438943894389439e-05,
      "loss": 0.3076,
      "step": 680
    },
    {
      "epoch": 0.5693069306930693,
      "grad_norm": 2.286944627761841,
      "learning_rate": 4.430693069306931e-05,
      "loss": 0.2665,
      "step": 690
    },
    {
      "epoch": 0.5775577557755776,
      "grad_norm": 2.3126585483551025,
      "learning_rate": 4.4224422442244226e-05,
      "loss": 0.2858,
      "step": 700
    },
    {
      "epoch": 0.5858085808580858,
      "grad_norm": 3.7350516319274902,
      "learning_rate": 4.414191419141914e-05,
      "loss": 0.2343,
      "step": 710
    },
    {
      "epoch": 0.594059405940594,
      "grad_norm": 4.889675140380859,
      "learning_rate": 4.405940594059406e-05,
      "loss": 0.2805,
      "step": 720
    },
    {
      "epoch": 0.6023102310231023,
      "grad_norm": 3.552568197250366,
      "learning_rate": 4.397689768976898e-05,
      "loss": 0.317,
      "step": 730
    },
    {
      "epoch": 0.6105610561056105,
      "grad_norm": 5.249702453613281,
      "learning_rate": 4.389438943894389e-05,
      "loss": 0.2673,
      "step": 740
    },
    {
      "epoch": 0.6188118811881188,
      "grad_norm": 4.900967121124268,
      "learning_rate": 4.3811881188118816e-05,
      "loss": 0.2746,
      "step": 750
    },
    {
      "epoch": 0.6270627062706271,
      "grad_norm": 2.6552529335021973,
      "learning_rate": 4.372937293729373e-05,
      "loss": 0.2763,
      "step": 760
    },
    {
      "epoch": 0.6353135313531353,
      "grad_norm": 2.8306593894958496,
      "learning_rate": 4.364686468646865e-05,
      "loss": 0.3785,
      "step": 770
    },
    {
      "epoch": 0.6435643564356436,
      "grad_norm": 1.7825862169265747,
      "learning_rate": 4.3564356435643565e-05,
      "loss": 0.2518,
      "step": 780
    },
    {
      "epoch": 0.6518151815181518,
      "grad_norm": 2.8313441276550293,
      "learning_rate": 4.348184818481849e-05,
      "loss": 0.303,
      "step": 790
    },
    {
      "epoch": 0.6600660066006601,
      "grad_norm": 2.7232189178466797,
      "learning_rate": 4.33993399339934e-05,
      "loss": 0.2896,
      "step": 800
    },
    {
      "epoch": 0.6683168316831684,
      "grad_norm": 3.3861377239227295,
      "learning_rate": 4.331683168316832e-05,
      "loss": 0.2964,
      "step": 810
    },
    {
      "epoch": 0.6765676567656765,
      "grad_norm": 3.4801456928253174,
      "learning_rate": 4.323432343234324e-05,
      "loss": 0.2967,
      "step": 820
    },
    {
      "epoch": 0.6848184818481848,
      "grad_norm": 3.927963972091675,
      "learning_rate": 4.3151815181518154e-05,
      "loss": 0.293,
      "step": 830
    },
    {
      "epoch": 0.693069306930693,
      "grad_norm": 4.702145576477051,
      "learning_rate": 4.306930693069307e-05,
      "loss": 0.3166,
      "step": 840
    },
    {
      "epoch": 0.7013201320132013,
      "grad_norm": 3.54081130027771,
      "learning_rate": 4.298679867986799e-05,
      "loss": 0.2723,
      "step": 850
    },
    {
      "epoch": 0.7095709570957096,
      "grad_norm": 3.4874985218048096,
      "learning_rate": 4.2904290429042904e-05,
      "loss": 0.3185,
      "step": 860
    },
    {
      "epoch": 0.7178217821782178,
      "grad_norm": 3.1837973594665527,
      "learning_rate": 4.282178217821782e-05,
      "loss": 0.2778,
      "step": 870
    },
    {
      "epoch": 0.7260726072607261,
      "grad_norm": 4.443111896514893,
      "learning_rate": 4.2739273927392744e-05,
      "loss": 0.2828,
      "step": 880
    },
    {
      "epoch": 0.7343234323432343,
      "grad_norm": 3.7359275817871094,
      "learning_rate": 4.265676567656766e-05,
      "loss": 0.2857,
      "step": 890
    },
    {
      "epoch": 0.7425742574257426,
      "grad_norm": 2.635260581970215,
      "learning_rate": 4.257425742574258e-05,
      "loss": 0.2713,
      "step": 900
    },
    {
      "epoch": 0.7508250825082509,
      "grad_norm": 2.576046943664551,
      "learning_rate": 4.249174917491749e-05,
      "loss": 0.2663,
      "step": 910
    },
    {
      "epoch": 0.759075907590759,
      "grad_norm": 2.023496150970459,
      "learning_rate": 4.240924092409242e-05,
      "loss": 0.2468,
      "step": 920
    },
    {
      "epoch": 0.7673267326732673,
      "grad_norm": 2.626323938369751,
      "learning_rate": 4.2326732673267326e-05,
      "loss": 0.2498,
      "step": 930
    },
    {
      "epoch": 0.7755775577557755,
      "grad_norm": 2.2502496242523193,
      "learning_rate": 4.224422442244225e-05,
      "loss": 0.2662,
      "step": 940
    },
    {
      "epoch": 0.7838283828382838,
      "grad_norm": 2.599654197692871,
      "learning_rate": 4.2161716171617166e-05,
      "loss": 0.2907,
      "step": 950
    },
    {
      "epoch": 0.7920792079207921,
      "grad_norm": 8.227014541625977,
      "learning_rate": 4.207920792079208e-05,
      "loss": 0.2283,
      "step": 960
    },
    {
      "epoch": 0.8003300330033003,
      "grad_norm": 2.9013314247131348,
      "learning_rate": 4.1996699669967e-05,
      "loss": 0.2666,
      "step": 970
    },
    {
      "epoch": 0.8085808580858086,
      "grad_norm": 2.747849225997925,
      "learning_rate": 4.1914191419141916e-05,
      "loss": 0.3077,
      "step": 980
    },
    {
      "epoch": 0.8168316831683168,
      "grad_norm": 3.200547695159912,
      "learning_rate": 4.183168316831683e-05,
      "loss": 0.2448,
      "step": 990
    },
    {
      "epoch": 0.8250825082508251,
      "grad_norm": 3.3533201217651367,
      "learning_rate": 4.174917491749175e-05,
      "loss": 0.2691,
      "step": 1000
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 4.071969032287598,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.2367,
      "step": 1010
    },
    {
      "epoch": 0.8415841584158416,
      "grad_norm": 2.587905168533325,
      "learning_rate": 4.158415841584158e-05,
      "loss": 0.2983,
      "step": 1020
    },
    {
      "epoch": 0.8498349834983498,
      "grad_norm": 2.1035938262939453,
      "learning_rate": 4.1501650165016505e-05,
      "loss": 0.2271,
      "step": 1030
    },
    {
      "epoch": 0.858085808580858,
      "grad_norm": 2.8364017009735107,
      "learning_rate": 4.141914191419142e-05,
      "loss": 0.2439,
      "step": 1040
    },
    {
      "epoch": 0.8663366336633663,
      "grad_norm": 2.768667221069336,
      "learning_rate": 4.133663366336634e-05,
      "loss": 0.2771,
      "step": 1050
    },
    {
      "epoch": 0.8745874587458746,
      "grad_norm": 4.471379280090332,
      "learning_rate": 4.1254125412541255e-05,
      "loss": 0.3101,
      "step": 1060
    },
    {
      "epoch": 0.8828382838283828,
      "grad_norm": 3.113469123840332,
      "learning_rate": 4.117161716171618e-05,
      "loss": 0.2343,
      "step": 1070
    },
    {
      "epoch": 0.8910891089108911,
      "grad_norm": 1.8566216230392456,
      "learning_rate": 4.108910891089109e-05,
      "loss": 0.2755,
      "step": 1080
    },
    {
      "epoch": 0.8993399339933993,
      "grad_norm": 2.4191701412200928,
      "learning_rate": 4.100660066006601e-05,
      "loss": 0.2681,
      "step": 1090
    },
    {
      "epoch": 0.9075907590759076,
      "grad_norm": 3.4665684700012207,
      "learning_rate": 4.092409240924093e-05,
      "loss": 0.2501,
      "step": 1100
    },
    {
      "epoch": 0.9158415841584159,
      "grad_norm": 2.2138452529907227,
      "learning_rate": 4.0841584158415844e-05,
      "loss": 0.2514,
      "step": 1110
    },
    {
      "epoch": 0.9240924092409241,
      "grad_norm": 3.4746181964874268,
      "learning_rate": 4.075907590759076e-05,
      "loss": 0.2164,
      "step": 1120
    },
    {
      "epoch": 0.9323432343234324,
      "grad_norm": 3.488661050796509,
      "learning_rate": 4.067656765676568e-05,
      "loss": 0.2866,
      "step": 1130
    },
    {
      "epoch": 0.9405940594059405,
      "grad_norm": 3.9014949798583984,
      "learning_rate": 4.05940594059406e-05,
      "loss": 0.2598,
      "step": 1140
    },
    {
      "epoch": 0.9488448844884488,
      "grad_norm": 2.9151952266693115,
      "learning_rate": 4.051155115511551e-05,
      "loss": 0.2797,
      "step": 1150
    },
    {
      "epoch": 0.9570957095709571,
      "grad_norm": 3.342883348464966,
      "learning_rate": 4.042904290429043e-05,
      "loss": 0.2029,
      "step": 1160
    },
    {
      "epoch": 0.9653465346534653,
      "grad_norm": 3.970407247543335,
      "learning_rate": 4.034653465346535e-05,
      "loss": 0.2309,
      "step": 1170
    },
    {
      "epoch": 0.9735973597359736,
      "grad_norm": 4.070700168609619,
      "learning_rate": 4.0264026402640266e-05,
      "loss": 0.2869,
      "step": 1180
    },
    {
      "epoch": 0.9818481848184818,
      "grad_norm": 1.7069751024246216,
      "learning_rate": 4.018151815181518e-05,
      "loss": 0.2296,
      "step": 1190
    },
    {
      "epoch": 0.9900990099009901,
      "grad_norm": 3.490600824356079,
      "learning_rate": 4.0099009900990106e-05,
      "loss": 0.2125,
      "step": 1200
    },
    {
      "epoch": 0.9983498349834984,
      "grad_norm": 3.510986328125,
      "learning_rate": 4.0016501650165016e-05,
      "loss": 0.2764,
      "step": 1210
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.23697902262210846,
      "eval_runtime": 27.8953,
      "eval_samples_per_second": 86.896,
      "eval_steps_per_second": 10.862,
      "step": 1212
    },
    {
      "epoch": 1.0066006600660067,
      "grad_norm": 3.3347978591918945,
      "learning_rate": 3.993399339933994e-05,
      "loss": 0.1973,
      "step": 1220
    },
    {
      "epoch": 1.0148514851485149,
      "grad_norm": 2.099229574203491,
      "learning_rate": 3.9851485148514856e-05,
      "loss": 0.1966,
      "step": 1230
    },
    {
      "epoch": 1.023102310231023,
      "grad_norm": 2.939683198928833,
      "learning_rate": 3.976897689768977e-05,
      "loss": 0.1954,
      "step": 1240
    },
    {
      "epoch": 1.0313531353135315,
      "grad_norm": 4.076406478881836,
      "learning_rate": 3.968646864686469e-05,
      "loss": 0.2064,
      "step": 1250
    },
    {
      "epoch": 1.0396039603960396,
      "grad_norm": 2.764108896255493,
      "learning_rate": 3.9603960396039605e-05,
      "loss": 0.1691,
      "step": 1260
    },
    {
      "epoch": 1.0478547854785478,
      "grad_norm": 5.783308506011963,
      "learning_rate": 3.952145214521452e-05,
      "loss": 0.2273,
      "step": 1270
    },
    {
      "epoch": 1.056105610561056,
      "grad_norm": 3.5948805809020996,
      "learning_rate": 3.943894389438944e-05,
      "loss": 0.2533,
      "step": 1280
    },
    {
      "epoch": 1.0643564356435644,
      "grad_norm": 2.821617364883423,
      "learning_rate": 3.935643564356436e-05,
      "loss": 0.234,
      "step": 1290
    },
    {
      "epoch": 1.0726072607260726,
      "grad_norm": 3.6622252464294434,
      "learning_rate": 3.927392739273927e-05,
      "loss": 0.2599,
      "step": 1300
    },
    {
      "epoch": 1.0808580858085808,
      "grad_norm": 3.664005994796753,
      "learning_rate": 3.9191419141914195e-05,
      "loss": 0.2074,
      "step": 1310
    },
    {
      "epoch": 1.0891089108910892,
      "grad_norm": 2.206467390060425,
      "learning_rate": 3.910891089108911e-05,
      "loss": 0.2658,
      "step": 1320
    },
    {
      "epoch": 1.0973597359735974,
      "grad_norm": 2.0819473266601562,
      "learning_rate": 3.902640264026403e-05,
      "loss": 0.1828,
      "step": 1330
    },
    {
      "epoch": 1.1056105610561056,
      "grad_norm": 2.524416923522949,
      "learning_rate": 3.8943894389438944e-05,
      "loss": 0.1995,
      "step": 1340
    },
    {
      "epoch": 1.113861386138614,
      "grad_norm": 2.97489857673645,
      "learning_rate": 3.886138613861387e-05,
      "loss": 0.2013,
      "step": 1350
    },
    {
      "epoch": 1.1221122112211221,
      "grad_norm": 2.386868953704834,
      "learning_rate": 3.877887788778878e-05,
      "loss": 0.2118,
      "step": 1360
    },
    {
      "epoch": 1.1303630363036303,
      "grad_norm": 2.5368640422821045,
      "learning_rate": 3.86963696369637e-05,
      "loss": 0.2122,
      "step": 1370
    },
    {
      "epoch": 1.1386138613861387,
      "grad_norm": 2.8667829036712646,
      "learning_rate": 3.861386138613862e-05,
      "loss": 0.2006,
      "step": 1380
    },
    {
      "epoch": 1.146864686468647,
      "grad_norm": 3.2885663509368896,
      "learning_rate": 3.8531353135313534e-05,
      "loss": 0.2047,
      "step": 1390
    },
    {
      "epoch": 1.155115511551155,
      "grad_norm": 2.5617194175720215,
      "learning_rate": 3.844884488448845e-05,
      "loss": 0.2183,
      "step": 1400
    },
    {
      "epoch": 1.1633663366336633,
      "grad_norm": 3.526552200317383,
      "learning_rate": 3.8366336633663367e-05,
      "loss": 0.1961,
      "step": 1410
    },
    {
      "epoch": 1.1716171617161717,
      "grad_norm": 1.884110927581787,
      "learning_rate": 3.828382838283829e-05,
      "loss": 0.2068,
      "step": 1420
    },
    {
      "epoch": 1.1798679867986799,
      "grad_norm": 2.4750516414642334,
      "learning_rate": 3.82013201320132e-05,
      "loss": 0.1781,
      "step": 1430
    },
    {
      "epoch": 1.188118811881188,
      "grad_norm": 2.0336225032806396,
      "learning_rate": 3.811881188118812e-05,
      "loss": 0.1895,
      "step": 1440
    },
    {
      "epoch": 1.1963696369636962,
      "grad_norm": 1.9425657987594604,
      "learning_rate": 3.803630363036304e-05,
      "loss": 0.1946,
      "step": 1450
    },
    {
      "epoch": 1.2046204620462047,
      "grad_norm": 2.1449599266052246,
      "learning_rate": 3.7953795379537956e-05,
      "loss": 0.1761,
      "step": 1460
    },
    {
      "epoch": 1.2128712871287128,
      "grad_norm": 3.546520233154297,
      "learning_rate": 3.787128712871287e-05,
      "loss": 0.227,
      "step": 1470
    },
    {
      "epoch": 1.221122112211221,
      "grad_norm": 1.8416988849639893,
      "learning_rate": 3.7788778877887796e-05,
      "loss": 0.1836,
      "step": 1480
    },
    {
      "epoch": 1.2293729372937294,
      "grad_norm": 3.1647403240203857,
      "learning_rate": 3.7706270627062705e-05,
      "loss": 0.1704,
      "step": 1490
    },
    {
      "epoch": 1.2376237623762376,
      "grad_norm": 2.674891948699951,
      "learning_rate": 3.762376237623763e-05,
      "loss": 0.2256,
      "step": 1500
    },
    {
      "epoch": 1.2458745874587458,
      "grad_norm": 3.499100685119629,
      "learning_rate": 3.7541254125412545e-05,
      "loss": 0.2098,
      "step": 1510
    },
    {
      "epoch": 1.2541254125412542,
      "grad_norm": 2.5484366416931152,
      "learning_rate": 3.745874587458746e-05,
      "loss": 0.2144,
      "step": 1520
    },
    {
      "epoch": 1.2623762376237624,
      "grad_norm": 2.0458881855010986,
      "learning_rate": 3.737623762376238e-05,
      "loss": 0.2622,
      "step": 1530
    },
    {
      "epoch": 1.2706270627062706,
      "grad_norm": 2.4488673210144043,
      "learning_rate": 3.7293729372937295e-05,
      "loss": 0.1954,
      "step": 1540
    },
    {
      "epoch": 1.278877887788779,
      "grad_norm": 1.7031654119491577,
      "learning_rate": 3.721122112211221e-05,
      "loss": 0.1606,
      "step": 1550
    },
    {
      "epoch": 1.2871287128712872,
      "grad_norm": 2.054713726043701,
      "learning_rate": 3.712871287128713e-05,
      "loss": 0.1707,
      "step": 1560
    },
    {
      "epoch": 1.2953795379537953,
      "grad_norm": 3.7028462886810303,
      "learning_rate": 3.704620462046205e-05,
      "loss": 0.1766,
      "step": 1570
    },
    {
      "epoch": 1.3036303630363038,
      "grad_norm": 2.8581185340881348,
      "learning_rate": 3.696369636963696e-05,
      "loss": 0.1684,
      "step": 1580
    },
    {
      "epoch": 1.311881188118812,
      "grad_norm": 2.2040724754333496,
      "learning_rate": 3.6881188118811884e-05,
      "loss": 0.2175,
      "step": 1590
    },
    {
      "epoch": 1.3201320132013201,
      "grad_norm": 1.7672408819198608,
      "learning_rate": 3.67986798679868e-05,
      "loss": 0.1826,
      "step": 1600
    },
    {
      "epoch": 1.3283828382838283,
      "grad_norm": 4.183960914611816,
      "learning_rate": 3.671617161716172e-05,
      "loss": 0.1722,
      "step": 1610
    },
    {
      "epoch": 1.3366336633663367,
      "grad_norm": 2.4733245372772217,
      "learning_rate": 3.6633663366336634e-05,
      "loss": 0.204,
      "step": 1620
    },
    {
      "epoch": 1.344884488448845,
      "grad_norm": 2.2593774795532227,
      "learning_rate": 3.655115511551156e-05,
      "loss": 0.1868,
      "step": 1630
    },
    {
      "epoch": 1.353135313531353,
      "grad_norm": 2.62953782081604,
      "learning_rate": 3.646864686468647e-05,
      "loss": 0.2095,
      "step": 1640
    },
    {
      "epoch": 1.3613861386138613,
      "grad_norm": 2.806962013244629,
      "learning_rate": 3.638613861386139e-05,
      "loss": 0.1904,
      "step": 1650
    },
    {
      "epoch": 1.3696369636963697,
      "grad_norm": 2.0139665603637695,
      "learning_rate": 3.6303630363036307e-05,
      "loss": 0.2219,
      "step": 1660
    },
    {
      "epoch": 1.3778877887788779,
      "grad_norm": 4.485980033874512,
      "learning_rate": 3.622112211221122e-05,
      "loss": 0.2201,
      "step": 1670
    },
    {
      "epoch": 1.386138613861386,
      "grad_norm": 3.126142740249634,
      "learning_rate": 3.613861386138614e-05,
      "loss": 0.2034,
      "step": 1680
    },
    {
      "epoch": 1.3943894389438944,
      "grad_norm": 2.5733959674835205,
      "learning_rate": 3.6056105610561056e-05,
      "loss": 0.2232,
      "step": 1690
    },
    {
      "epoch": 1.4026402640264026,
      "grad_norm": 3.27604079246521,
      "learning_rate": 3.597359735973598e-05,
      "loss": 0.2095,
      "step": 1700
    },
    {
      "epoch": 1.4108910891089108,
      "grad_norm": 2.8829479217529297,
      "learning_rate": 3.589108910891089e-05,
      "loss": 0.1712,
      "step": 1710
    },
    {
      "epoch": 1.4191419141914192,
      "grad_norm": 0.9955251216888428,
      "learning_rate": 3.580858085808581e-05,
      "loss": 0.1913,
      "step": 1720
    },
    {
      "epoch": 1.4273927392739274,
      "grad_norm": 3.2165627479553223,
      "learning_rate": 3.572607260726073e-05,
      "loss": 0.2004,
      "step": 1730
    },
    {
      "epoch": 1.4356435643564356,
      "grad_norm": 3.2700564861297607,
      "learning_rate": 3.5643564356435645e-05,
      "loss": 0.1466,
      "step": 1740
    },
    {
      "epoch": 1.443894389438944,
      "grad_norm": 2.3610308170318604,
      "learning_rate": 3.556105610561056e-05,
      "loss": 0.2073,
      "step": 1750
    },
    {
      "epoch": 1.4521452145214522,
      "grad_norm": 2.8108134269714355,
      "learning_rate": 3.5478547854785485e-05,
      "loss": 0.2038,
      "step": 1760
    },
    {
      "epoch": 1.4603960396039604,
      "grad_norm": 2.706259250640869,
      "learning_rate": 3.5396039603960395e-05,
      "loss": 0.2136,
      "step": 1770
    },
    {
      "epoch": 1.4686468646864688,
      "grad_norm": 3.772040367126465,
      "learning_rate": 3.531353135313532e-05,
      "loss": 0.1832,
      "step": 1780
    },
    {
      "epoch": 1.476897689768977,
      "grad_norm": 2.0023386478424072,
      "learning_rate": 3.5231023102310235e-05,
      "loss": 0.2035,
      "step": 1790
    },
    {
      "epoch": 1.4851485148514851,
      "grad_norm": 2.588557243347168,
      "learning_rate": 3.514851485148515e-05,
      "loss": 0.2033,
      "step": 1800
    },
    {
      "epoch": 1.4933993399339933,
      "grad_norm": 3.708026647567749,
      "learning_rate": 3.506600660066007e-05,
      "loss": 0.2079,
      "step": 1810
    },
    {
      "epoch": 1.5016501650165015,
      "grad_norm": 2.775724411010742,
      "learning_rate": 3.4983498349834984e-05,
      "loss": 0.1736,
      "step": 1820
    },
    {
      "epoch": 1.50990099009901,
      "grad_norm": 3.417281150817871,
      "learning_rate": 3.49009900990099e-05,
      "loss": 0.1937,
      "step": 1830
    },
    {
      "epoch": 1.5181518151815183,
      "grad_norm": 2.9760968685150146,
      "learning_rate": 3.481848184818482e-05,
      "loss": 0.2356,
      "step": 1840
    },
    {
      "epoch": 1.5264026402640263,
      "grad_norm": 3.3363747596740723,
      "learning_rate": 3.473597359735974e-05,
      "loss": 0.1641,
      "step": 1850
    },
    {
      "epoch": 1.5346534653465347,
      "grad_norm": 3.4850025177001953,
      "learning_rate": 3.465346534653465e-05,
      "loss": 0.1968,
      "step": 1860
    },
    {
      "epoch": 1.5429042904290429,
      "grad_norm": 2.6619303226470947,
      "learning_rate": 3.4570957095709574e-05,
      "loss": 0.1968,
      "step": 1870
    },
    {
      "epoch": 1.551155115511551,
      "grad_norm": 2.5311951637268066,
      "learning_rate": 3.448844884488449e-05,
      "loss": 0.1731,
      "step": 1880
    },
    {
      "epoch": 1.5594059405940595,
      "grad_norm": 5.844417095184326,
      "learning_rate": 3.440594059405941e-05,
      "loss": 0.1936,
      "step": 1890
    },
    {
      "epoch": 1.5676567656765676,
      "grad_norm": 2.6417429447174072,
      "learning_rate": 3.432343234323432e-05,
      "loss": 0.1774,
      "step": 1900
    },
    {
      "epoch": 1.5759075907590758,
      "grad_norm": 3.464123249053955,
      "learning_rate": 3.4240924092409246e-05,
      "loss": 0.1545,
      "step": 1910
    },
    {
      "epoch": 1.5841584158415842,
      "grad_norm": 2.9826245307922363,
      "learning_rate": 3.415841584158416e-05,
      "loss": 0.1973,
      "step": 1920
    },
    {
      "epoch": 1.5924092409240924,
      "grad_norm": 2.398973226547241,
      "learning_rate": 3.407590759075908e-05,
      "loss": 0.1972,
      "step": 1930
    },
    {
      "epoch": 1.6006600660066006,
      "grad_norm": 5.132493019104004,
      "learning_rate": 3.3993399339933996e-05,
      "loss": 0.195,
      "step": 1940
    },
    {
      "epoch": 1.608910891089109,
      "grad_norm": 2.5134365558624268,
      "learning_rate": 3.391089108910891e-05,
      "loss": 0.2214,
      "step": 1950
    },
    {
      "epoch": 1.6171617161716172,
      "grad_norm": 2.9752135276794434,
      "learning_rate": 3.382838283828383e-05,
      "loss": 0.1785,
      "step": 1960
    },
    {
      "epoch": 1.6254125412541254,
      "grad_norm": 3.4629883766174316,
      "learning_rate": 3.3745874587458746e-05,
      "loss": 0.1797,
      "step": 1970
    },
    {
      "epoch": 1.6336633663366338,
      "grad_norm": 2.3258750438690186,
      "learning_rate": 3.366336633663367e-05,
      "loss": 0.1626,
      "step": 1980
    },
    {
      "epoch": 1.641914191419142,
      "grad_norm": 3.579918384552002,
      "learning_rate": 3.358085808580858e-05,
      "loss": 0.1876,
      "step": 1990
    },
    {
      "epoch": 1.6501650165016502,
      "grad_norm": 2.6857166290283203,
      "learning_rate": 3.34983498349835e-05,
      "loss": 0.1991,
      "step": 2000
    },
    {
      "epoch": 1.6584158415841586,
      "grad_norm": 3.064842939376831,
      "learning_rate": 3.341584158415842e-05,
      "loss": 0.1787,
      "step": 2010
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 2.327547550201416,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.1755,
      "step": 2020
    },
    {
      "epoch": 1.674917491749175,
      "grad_norm": 3.2021608352661133,
      "learning_rate": 3.325082508250825e-05,
      "loss": 0.1808,
      "step": 2030
    },
    {
      "epoch": 1.6831683168316833,
      "grad_norm": 5.394594192504883,
      "learning_rate": 3.3168316831683175e-05,
      "loss": 0.219,
      "step": 2040
    },
    {
      "epoch": 1.6914191419141913,
      "grad_norm": 2.671009063720703,
      "learning_rate": 3.3085808580858084e-05,
      "loss": 0.1737,
      "step": 2050
    },
    {
      "epoch": 1.6996699669966997,
      "grad_norm": 3.163496494293213,
      "learning_rate": 3.300330033003301e-05,
      "loss": 0.1939,
      "step": 2060
    },
    {
      "epoch": 1.7079207920792079,
      "grad_norm": 3.4059062004089355,
      "learning_rate": 3.2920792079207924e-05,
      "loss": 0.209,
      "step": 2070
    },
    {
      "epoch": 1.716171617161716,
      "grad_norm": 2.9655230045318604,
      "learning_rate": 3.283828382838284e-05,
      "loss": 0.1802,
      "step": 2080
    },
    {
      "epoch": 1.7244224422442245,
      "grad_norm": 3.04223370552063,
      "learning_rate": 3.275577557755776e-05,
      "loss": 0.1865,
      "step": 2090
    },
    {
      "epoch": 1.7326732673267327,
      "grad_norm": 3.448770761489868,
      "learning_rate": 3.2673267326732674e-05,
      "loss": 0.1595,
      "step": 2100
    },
    {
      "epoch": 1.7409240924092408,
      "grad_norm": 2.5744640827178955,
      "learning_rate": 3.259075907590759e-05,
      "loss": 0.1782,
      "step": 2110
    },
    {
      "epoch": 1.7491749174917492,
      "grad_norm": 2.9918301105499268,
      "learning_rate": 3.250825082508251e-05,
      "loss": 0.2182,
      "step": 2120
    },
    {
      "epoch": 1.7574257425742574,
      "grad_norm": 2.429736852645874,
      "learning_rate": 3.242574257425743e-05,
      "loss": 0.2111,
      "step": 2130
    },
    {
      "epoch": 1.7656765676567656,
      "grad_norm": 3.5725607872009277,
      "learning_rate": 3.234323432343234e-05,
      "loss": 0.2159,
      "step": 2140
    },
    {
      "epoch": 1.773927392739274,
      "grad_norm": 2.0950706005096436,
      "learning_rate": 3.226072607260726e-05,
      "loss": 0.2123,
      "step": 2150
    },
    {
      "epoch": 1.7821782178217822,
      "grad_norm": 2.1521494388580322,
      "learning_rate": 3.217821782178218e-05,
      "loss": 0.1939,
      "step": 2160
    },
    {
      "epoch": 1.7904290429042904,
      "grad_norm": 2.499598503112793,
      "learning_rate": 3.20957095709571e-05,
      "loss": 0.2269,
      "step": 2170
    },
    {
      "epoch": 1.7986798679867988,
      "grad_norm": 2.768183946609497,
      "learning_rate": 3.201320132013201e-05,
      "loss": 0.2104,
      "step": 2180
    },
    {
      "epoch": 1.806930693069307,
      "grad_norm": 2.155747652053833,
      "learning_rate": 3.1930693069306936e-05,
      "loss": 0.2255,
      "step": 2190
    },
    {
      "epoch": 1.8151815181518152,
      "grad_norm": 2.828702926635742,
      "learning_rate": 3.184818481848185e-05,
      "loss": 0.1724,
      "step": 2200
    },
    {
      "epoch": 1.8234323432343236,
      "grad_norm": 1.471069097518921,
      "learning_rate": 3.176567656765677e-05,
      "loss": 0.1617,
      "step": 2210
    },
    {
      "epoch": 1.8316831683168315,
      "grad_norm": 2.871699810028076,
      "learning_rate": 3.1683168316831686e-05,
      "loss": 0.1805,
      "step": 2220
    },
    {
      "epoch": 1.83993399339934,
      "grad_norm": 2.2926740646362305,
      "learning_rate": 3.16006600660066e-05,
      "loss": 0.1753,
      "step": 2230
    },
    {
      "epoch": 1.8481848184818483,
      "grad_norm": 2.0374417304992676,
      "learning_rate": 3.151815181518152e-05,
      "loss": 0.2028,
      "step": 2240
    },
    {
      "epoch": 1.8564356435643563,
      "grad_norm": 3.8870346546173096,
      "learning_rate": 3.1435643564356435e-05,
      "loss": 0.177,
      "step": 2250
    },
    {
      "epoch": 1.8646864686468647,
      "grad_norm": 3.696505308151245,
      "learning_rate": 3.135313531353136e-05,
      "loss": 0.1351,
      "step": 2260
    },
    {
      "epoch": 1.872937293729373,
      "grad_norm": 3.6069371700286865,
      "learning_rate": 3.127062706270627e-05,
      "loss": 0.1706,
      "step": 2270
    },
    {
      "epoch": 1.881188118811881,
      "grad_norm": 2.555967330932617,
      "learning_rate": 3.118811881188119e-05,
      "loss": 0.1905,
      "step": 2280
    },
    {
      "epoch": 1.8894389438943895,
      "grad_norm": 8.269207954406738,
      "learning_rate": 3.110561056105611e-05,
      "loss": 0.1991,
      "step": 2290
    },
    {
      "epoch": 1.8976897689768977,
      "grad_norm": 1.714110255241394,
      "learning_rate": 3.1023102310231024e-05,
      "loss": 0.1787,
      "step": 2300
    },
    {
      "epoch": 1.9059405940594059,
      "grad_norm": 3.4873154163360596,
      "learning_rate": 3.094059405940594e-05,
      "loss": 0.2205,
      "step": 2310
    },
    {
      "epoch": 1.9141914191419143,
      "grad_norm": 2.979966878890991,
      "learning_rate": 3.0858085808580864e-05,
      "loss": 0.1882,
      "step": 2320
    },
    {
      "epoch": 1.9224422442244224,
      "grad_norm": 4.250412464141846,
      "learning_rate": 3.0775577557755774e-05,
      "loss": 0.2017,
      "step": 2330
    },
    {
      "epoch": 1.9306930693069306,
      "grad_norm": 3.348344326019287,
      "learning_rate": 3.06930693069307e-05,
      "loss": 0.1812,
      "step": 2340
    },
    {
      "epoch": 1.938943894389439,
      "grad_norm": 2.4278626441955566,
      "learning_rate": 3.0610561056105614e-05,
      "loss": 0.2159,
      "step": 2350
    },
    {
      "epoch": 1.9471947194719472,
      "grad_norm": 3.8224923610687256,
      "learning_rate": 3.052805280528053e-05,
      "loss": 0.1859,
      "step": 2360
    },
    {
      "epoch": 1.9554455445544554,
      "grad_norm": 3.5001542568206787,
      "learning_rate": 3.0445544554455447e-05,
      "loss": 0.1499,
      "step": 2370
    },
    {
      "epoch": 1.9636963696369638,
      "grad_norm": 3.4222426414489746,
      "learning_rate": 3.0363036303630367e-05,
      "loss": 0.1656,
      "step": 2380
    },
    {
      "epoch": 1.971947194719472,
      "grad_norm": 3.679993152618408,
      "learning_rate": 3.028052805280528e-05,
      "loss": 0.2192,
      "step": 2390
    },
    {
      "epoch": 1.9801980198019802,
      "grad_norm": 2.1005043983459473,
      "learning_rate": 3.01980198019802e-05,
      "loss": 0.1996,
      "step": 2400
    },
    {
      "epoch": 1.9884488448844886,
      "grad_norm": 2.8222367763519287,
      "learning_rate": 3.0115511551155116e-05,
      "loss": 0.1915,
      "step": 2410
    },
    {
      "epoch": 1.9966996699669965,
      "grad_norm": 2.3420820236206055,
      "learning_rate": 3.0033003300330036e-05,
      "loss": 0.1787,
      "step": 2420
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.2057664841413498,
      "eval_runtime": 22.9747,
      "eval_samples_per_second": 105.507,
      "eval_steps_per_second": 13.188,
      "step": 2424
    },
    {
      "epoch": 2.004950495049505,
      "grad_norm": 4.2701215744018555,
      "learning_rate": 2.995049504950495e-05,
      "loss": 0.1305,
      "step": 2430
    },
    {
      "epoch": 2.0132013201320134,
      "grad_norm": 2.1860947608947754,
      "learning_rate": 2.986798679867987e-05,
      "loss": 0.1615,
      "step": 2440
    },
    {
      "epoch": 2.0214521452145213,
      "grad_norm": 2.8905208110809326,
      "learning_rate": 2.978547854785479e-05,
      "loss": 0.1107,
      "step": 2450
    },
    {
      "epoch": 2.0297029702970297,
      "grad_norm": 2.7223873138427734,
      "learning_rate": 2.9702970297029702e-05,
      "loss": 0.1459,
      "step": 2460
    },
    {
      "epoch": 2.037953795379538,
      "grad_norm": 3.227264881134033,
      "learning_rate": 2.9620462046204622e-05,
      "loss": 0.1273,
      "step": 2470
    },
    {
      "epoch": 2.046204620462046,
      "grad_norm": 2.3177132606506348,
      "learning_rate": 2.9537953795379542e-05,
      "loss": 0.1575,
      "step": 2480
    },
    {
      "epoch": 2.0544554455445545,
      "grad_norm": 3.763923168182373,
      "learning_rate": 2.9455445544554455e-05,
      "loss": 0.1551,
      "step": 2490
    },
    {
      "epoch": 2.062706270627063,
      "grad_norm": 2.4609789848327637,
      "learning_rate": 2.9372937293729375e-05,
      "loss": 0.1273,
      "step": 2500
    },
    {
      "epoch": 2.070957095709571,
      "grad_norm": 3.032142162322998,
      "learning_rate": 2.9290429042904295e-05,
      "loss": 0.128,
      "step": 2510
    },
    {
      "epoch": 2.0792079207920793,
      "grad_norm": 3.5887677669525146,
      "learning_rate": 2.9207920792079208e-05,
      "loss": 0.1339,
      "step": 2520
    },
    {
      "epoch": 2.0874587458745877,
      "grad_norm": 3.1811277866363525,
      "learning_rate": 2.9125412541254128e-05,
      "loss": 0.1224,
      "step": 2530
    },
    {
      "epoch": 2.0957095709570956,
      "grad_norm": 2.1454381942749023,
      "learning_rate": 2.9042904290429045e-05,
      "loss": 0.1572,
      "step": 2540
    },
    {
      "epoch": 2.103960396039604,
      "grad_norm": 6.834484100341797,
      "learning_rate": 2.896039603960396e-05,
      "loss": 0.1656,
      "step": 2550
    },
    {
      "epoch": 2.112211221122112,
      "grad_norm": 3.0032005310058594,
      "learning_rate": 2.8877887788778878e-05,
      "loss": 0.1257,
      "step": 2560
    },
    {
      "epoch": 2.1204620462046204,
      "grad_norm": 2.5476250648498535,
      "learning_rate": 2.8795379537953797e-05,
      "loss": 0.1388,
      "step": 2570
    },
    {
      "epoch": 2.128712871287129,
      "grad_norm": 3.3404953479766846,
      "learning_rate": 2.871287128712871e-05,
      "loss": 0.108,
      "step": 2580
    },
    {
      "epoch": 2.136963696369637,
      "grad_norm": 1.8492380380630493,
      "learning_rate": 2.863036303630363e-05,
      "loss": 0.1505,
      "step": 2590
    },
    {
      "epoch": 2.145214521452145,
      "grad_norm": 3.735163927078247,
      "learning_rate": 2.854785478547855e-05,
      "loss": 0.1246,
      "step": 2600
    },
    {
      "epoch": 2.1534653465346536,
      "grad_norm": 1.3703639507293701,
      "learning_rate": 2.8465346534653464e-05,
      "loss": 0.1583,
      "step": 2610
    },
    {
      "epoch": 2.1617161716171616,
      "grad_norm": 3.413546562194824,
      "learning_rate": 2.8382838283828383e-05,
      "loss": 0.164,
      "step": 2620
    },
    {
      "epoch": 2.16996699669967,
      "grad_norm": 2.6536359786987305,
      "learning_rate": 2.8300330033003303e-05,
      "loss": 0.1541,
      "step": 2630
    },
    {
      "epoch": 2.1782178217821784,
      "grad_norm": 2.8752667903900146,
      "learning_rate": 2.8217821782178216e-05,
      "loss": 0.139,
      "step": 2640
    },
    {
      "epoch": 2.1864686468646863,
      "grad_norm": 2.1938893795013428,
      "learning_rate": 2.8135313531353136e-05,
      "loss": 0.1532,
      "step": 2650
    },
    {
      "epoch": 2.1947194719471947,
      "grad_norm": 1.8863040208816528,
      "learning_rate": 2.8052805280528056e-05,
      "loss": 0.1502,
      "step": 2660
    },
    {
      "epoch": 2.202970297029703,
      "grad_norm": 4.83604097366333,
      "learning_rate": 2.7970297029702973e-05,
      "loss": 0.11,
      "step": 2670
    },
    {
      "epoch": 2.211221122112211,
      "grad_norm": 2.3544867038726807,
      "learning_rate": 2.788778877887789e-05,
      "loss": 0.1244,
      "step": 2680
    },
    {
      "epoch": 2.2194719471947195,
      "grad_norm": 2.860579013824463,
      "learning_rate": 2.7805280528052806e-05,
      "loss": 0.1145,
      "step": 2690
    },
    {
      "epoch": 2.227722772277228,
      "grad_norm": 3.510267734527588,
      "learning_rate": 2.7722772277227726e-05,
      "loss": 0.1376,
      "step": 2700
    },
    {
      "epoch": 2.235973597359736,
      "grad_norm": 2.475618600845337,
      "learning_rate": 2.764026402640264e-05,
      "loss": 0.1339,
      "step": 2710
    },
    {
      "epoch": 2.2442244224422443,
      "grad_norm": 4.005702495574951,
      "learning_rate": 2.755775577557756e-05,
      "loss": 0.1042,
      "step": 2720
    },
    {
      "epoch": 2.2524752475247523,
      "grad_norm": 3.340972661972046,
      "learning_rate": 2.747524752475248e-05,
      "loss": 0.158,
      "step": 2730
    },
    {
      "epoch": 2.2607260726072607,
      "grad_norm": 3.1885128021240234,
      "learning_rate": 2.7392739273927392e-05,
      "loss": 0.1396,
      "step": 2740
    },
    {
      "epoch": 2.268976897689769,
      "grad_norm": 3.3282063007354736,
      "learning_rate": 2.731023102310231e-05,
      "loss": 0.1349,
      "step": 2750
    },
    {
      "epoch": 2.2772277227722775,
      "grad_norm": 4.839243412017822,
      "learning_rate": 2.722772277227723e-05,
      "loss": 0.1477,
      "step": 2760
    },
    {
      "epoch": 2.2854785478547854,
      "grad_norm": 3.547708034515381,
      "learning_rate": 2.7145214521452145e-05,
      "loss": 0.1577,
      "step": 2770
    },
    {
      "epoch": 2.293729372937294,
      "grad_norm": 1.9077332019805908,
      "learning_rate": 2.7062706270627065e-05,
      "loss": 0.1096,
      "step": 2780
    },
    {
      "epoch": 2.301980198019802,
      "grad_norm": 1.6834790706634521,
      "learning_rate": 2.6980198019801985e-05,
      "loss": 0.1254,
      "step": 2790
    },
    {
      "epoch": 2.31023102310231,
      "grad_norm": 1.6486507654190063,
      "learning_rate": 2.6897689768976898e-05,
      "loss": 0.103,
      "step": 2800
    },
    {
      "epoch": 2.3184818481848186,
      "grad_norm": 1.293908953666687,
      "learning_rate": 2.6815181518151818e-05,
      "loss": 0.1186,
      "step": 2810
    },
    {
      "epoch": 2.3267326732673266,
      "grad_norm": 2.3477377891540527,
      "learning_rate": 2.6732673267326734e-05,
      "loss": 0.1636,
      "step": 2820
    },
    {
      "epoch": 2.334983498349835,
      "grad_norm": 5.697864055633545,
      "learning_rate": 2.665016501650165e-05,
      "loss": 0.1346,
      "step": 2830
    },
    {
      "epoch": 2.3432343234323434,
      "grad_norm": 1.745226263999939,
      "learning_rate": 2.6567656765676567e-05,
      "loss": 0.1407,
      "step": 2840
    },
    {
      "epoch": 2.3514851485148514,
      "grad_norm": 2.3783740997314453,
      "learning_rate": 2.6485148514851487e-05,
      "loss": 0.1434,
      "step": 2850
    },
    {
      "epoch": 2.3597359735973598,
      "grad_norm": 2.0189402103424072,
      "learning_rate": 2.64026402640264e-05,
      "loss": 0.146,
      "step": 2860
    },
    {
      "epoch": 2.367986798679868,
      "grad_norm": 3.9360618591308594,
      "learning_rate": 2.632013201320132e-05,
      "loss": 0.1404,
      "step": 2870
    },
    {
      "epoch": 2.376237623762376,
      "grad_norm": 2.170377254486084,
      "learning_rate": 2.623762376237624e-05,
      "loss": 0.1387,
      "step": 2880
    },
    {
      "epoch": 2.3844884488448845,
      "grad_norm": 3.471306562423706,
      "learning_rate": 2.6155115511551153e-05,
      "loss": 0.1304,
      "step": 2890
    },
    {
      "epoch": 2.3927392739273925,
      "grad_norm": 2.4115021228790283,
      "learning_rate": 2.6072607260726073e-05,
      "loss": 0.1408,
      "step": 2900
    },
    {
      "epoch": 2.400990099009901,
      "grad_norm": 2.757812738418579,
      "learning_rate": 2.5990099009900993e-05,
      "loss": 0.1333,
      "step": 2910
    },
    {
      "epoch": 2.4092409240924093,
      "grad_norm": 2.1309313774108887,
      "learning_rate": 2.5907590759075913e-05,
      "loss": 0.1425,
      "step": 2920
    },
    {
      "epoch": 2.4174917491749177,
      "grad_norm": 1.8621761798858643,
      "learning_rate": 2.5825082508250826e-05,
      "loss": 0.1397,
      "step": 2930
    },
    {
      "epoch": 2.4257425742574257,
      "grad_norm": 2.937002658843994,
      "learning_rate": 2.5742574257425746e-05,
      "loss": 0.1092,
      "step": 2940
    },
    {
      "epoch": 2.433993399339934,
      "grad_norm": 2.481677770614624,
      "learning_rate": 2.5660066006600662e-05,
      "loss": 0.1417,
      "step": 2950
    },
    {
      "epoch": 2.442244224422442,
      "grad_norm": 3.391963481903076,
      "learning_rate": 2.557755775577558e-05,
      "loss": 0.1329,
      "step": 2960
    },
    {
      "epoch": 2.4504950495049505,
      "grad_norm": 3.474694013595581,
      "learning_rate": 2.5495049504950495e-05,
      "loss": 0.1508,
      "step": 2970
    },
    {
      "epoch": 2.458745874587459,
      "grad_norm": 3.364708185195923,
      "learning_rate": 2.5412541254125415e-05,
      "loss": 0.1268,
      "step": 2980
    },
    {
      "epoch": 2.466996699669967,
      "grad_norm": 2.0694193840026855,
      "learning_rate": 2.533003300330033e-05,
      "loss": 0.1444,
      "step": 2990
    },
    {
      "epoch": 2.4752475247524752,
      "grad_norm": 3.2038519382476807,
      "learning_rate": 2.5247524752475248e-05,
      "loss": 0.1563,
      "step": 3000
    },
    {
      "epoch": 2.4834983498349836,
      "grad_norm": 2.7048072814941406,
      "learning_rate": 2.5165016501650168e-05,
      "loss": 0.1162,
      "step": 3010
    },
    {
      "epoch": 2.4917491749174916,
      "grad_norm": 3.2455737590789795,
      "learning_rate": 2.508250825082508e-05,
      "loss": 0.0947,
      "step": 3020
    },
    {
      "epoch": 2.5,
      "grad_norm": 3.570631742477417,
      "learning_rate": 2.5e-05,
      "loss": 0.1155,
      "step": 3030
    },
    {
      "epoch": 2.5082508250825084,
      "grad_norm": 4.0006208419799805,
      "learning_rate": 2.4917491749174918e-05,
      "loss": 0.1206,
      "step": 3040
    },
    {
      "epoch": 2.5165016501650164,
      "grad_norm": 4.918198108673096,
      "learning_rate": 2.4834983498349838e-05,
      "loss": 0.1484,
      "step": 3050
    },
    {
      "epoch": 2.5247524752475248,
      "grad_norm": 3.1601388454437256,
      "learning_rate": 2.4752475247524754e-05,
      "loss": 0.1432,
      "step": 3060
    },
    {
      "epoch": 2.5330033003300327,
      "grad_norm": 1.0753968954086304,
      "learning_rate": 2.466996699669967e-05,
      "loss": 0.1264,
      "step": 3070
    },
    {
      "epoch": 2.541254125412541,
      "grad_norm": 3.2174172401428223,
      "learning_rate": 2.458745874587459e-05,
      "loss": 0.1206,
      "step": 3080
    },
    {
      "epoch": 2.5495049504950495,
      "grad_norm": 3.166520833969116,
      "learning_rate": 2.4504950495049507e-05,
      "loss": 0.1412,
      "step": 3090
    },
    {
      "epoch": 2.557755775577558,
      "grad_norm": 3.347200632095337,
      "learning_rate": 2.4422442244224424e-05,
      "loss": 0.1109,
      "step": 3100
    },
    {
      "epoch": 2.566006600660066,
      "grad_norm": 2.120037317276001,
      "learning_rate": 2.433993399339934e-05,
      "loss": 0.1203,
      "step": 3110
    },
    {
      "epoch": 2.5742574257425743,
      "grad_norm": 2.254131555557251,
      "learning_rate": 2.4257425742574257e-05,
      "loss": 0.1481,
      "step": 3120
    },
    {
      "epoch": 2.5825082508250823,
      "grad_norm": 5.625736236572266,
      "learning_rate": 2.4174917491749177e-05,
      "loss": 0.1334,
      "step": 3130
    },
    {
      "epoch": 2.5907590759075907,
      "grad_norm": 3.8404061794281006,
      "learning_rate": 2.4092409240924093e-05,
      "loss": 0.1245,
      "step": 3140
    },
    {
      "epoch": 2.599009900990099,
      "grad_norm": 4.044275760650635,
      "learning_rate": 2.400990099009901e-05,
      "loss": 0.1483,
      "step": 3150
    },
    {
      "epoch": 2.6072607260726075,
      "grad_norm": 1.9287227392196655,
      "learning_rate": 2.392739273927393e-05,
      "loss": 0.1401,
      "step": 3160
    },
    {
      "epoch": 2.6155115511551155,
      "grad_norm": 2.5234735012054443,
      "learning_rate": 2.3844884488448846e-05,
      "loss": 0.1473,
      "step": 3170
    },
    {
      "epoch": 2.623762376237624,
      "grad_norm": 3.3049211502075195,
      "learning_rate": 2.3762376237623762e-05,
      "loss": 0.15,
      "step": 3180
    },
    {
      "epoch": 2.632013201320132,
      "grad_norm": 2.08073353767395,
      "learning_rate": 2.3679867986798682e-05,
      "loss": 0.1338,
      "step": 3190
    },
    {
      "epoch": 2.6402640264026402,
      "grad_norm": 2.7106895446777344,
      "learning_rate": 2.35973597359736e-05,
      "loss": 0.1252,
      "step": 3200
    },
    {
      "epoch": 2.6485148514851486,
      "grad_norm": 2.3566157817840576,
      "learning_rate": 2.3514851485148515e-05,
      "loss": 0.1392,
      "step": 3210
    },
    {
      "epoch": 2.6567656765676566,
      "grad_norm": 1.6357207298278809,
      "learning_rate": 2.3432343234323435e-05,
      "loss": 0.1019,
      "step": 3220
    },
    {
      "epoch": 2.665016501650165,
      "grad_norm": 1.902308464050293,
      "learning_rate": 2.3349834983498352e-05,
      "loss": 0.1367,
      "step": 3230
    },
    {
      "epoch": 2.6732673267326734,
      "grad_norm": 3.087599277496338,
      "learning_rate": 2.326732673267327e-05,
      "loss": 0.1207,
      "step": 3240
    },
    {
      "epoch": 2.6815181518151814,
      "grad_norm": 1.8306301832199097,
      "learning_rate": 2.3184818481848185e-05,
      "loss": 0.1472,
      "step": 3250
    },
    {
      "epoch": 2.68976897689769,
      "grad_norm": 1.7302467823028564,
      "learning_rate": 2.31023102310231e-05,
      "loss": 0.1307,
      "step": 3260
    },
    {
      "epoch": 2.698019801980198,
      "grad_norm": 2.2317891120910645,
      "learning_rate": 2.301980198019802e-05,
      "loss": 0.1431,
      "step": 3270
    },
    {
      "epoch": 2.706270627062706,
      "grad_norm": 1.6473748683929443,
      "learning_rate": 2.2937293729372938e-05,
      "loss": 0.115,
      "step": 3280
    },
    {
      "epoch": 2.7145214521452146,
      "grad_norm": 2.870594024658203,
      "learning_rate": 2.2854785478547854e-05,
      "loss": 0.1156,
      "step": 3290
    },
    {
      "epoch": 2.7227722772277225,
      "grad_norm": 2.4657084941864014,
      "learning_rate": 2.2772277227722774e-05,
      "loss": 0.1186,
      "step": 3300
    },
    {
      "epoch": 2.731023102310231,
      "grad_norm": 2.444375514984131,
      "learning_rate": 2.268976897689769e-05,
      "loss": 0.1642,
      "step": 3310
    },
    {
      "epoch": 2.7392739273927393,
      "grad_norm": 2.4615659713745117,
      "learning_rate": 2.2607260726072607e-05,
      "loss": 0.1296,
      "step": 3320
    },
    {
      "epoch": 2.7475247524752477,
      "grad_norm": 3.434443473815918,
      "learning_rate": 2.2524752475247527e-05,
      "loss": 0.155,
      "step": 3330
    },
    {
      "epoch": 2.7557755775577557,
      "grad_norm": 2.1079158782958984,
      "learning_rate": 2.2442244224422444e-05,
      "loss": 0.1405,
      "step": 3340
    },
    {
      "epoch": 2.764026402640264,
      "grad_norm": 2.956040382385254,
      "learning_rate": 2.2359735973597364e-05,
      "loss": 0.1368,
      "step": 3350
    },
    {
      "epoch": 2.772277227722772,
      "grad_norm": 2.281676769256592,
      "learning_rate": 2.227722772277228e-05,
      "loss": 0.1173,
      "step": 3360
    },
    {
      "epoch": 2.7805280528052805,
      "grad_norm": 2.598724842071533,
      "learning_rate": 2.2194719471947197e-05,
      "loss": 0.1353,
      "step": 3370
    },
    {
      "epoch": 2.788778877887789,
      "grad_norm": 2.381281852722168,
      "learning_rate": 2.2112211221122113e-05,
      "loss": 0.1184,
      "step": 3380
    },
    {
      "epoch": 2.7970297029702973,
      "grad_norm": 1.826189637184143,
      "learning_rate": 2.202970297029703e-05,
      "loss": 0.1312,
      "step": 3390
    },
    {
      "epoch": 2.8052805280528053,
      "grad_norm": 2.6462998390197754,
      "learning_rate": 2.1947194719471946e-05,
      "loss": 0.1328,
      "step": 3400
    },
    {
      "epoch": 2.8135313531353137,
      "grad_norm": 3.1637020111083984,
      "learning_rate": 2.1864686468646866e-05,
      "loss": 0.1029,
      "step": 3410
    },
    {
      "epoch": 2.8217821782178216,
      "grad_norm": 3.6820895671844482,
      "learning_rate": 2.1782178217821783e-05,
      "loss": 0.1255,
      "step": 3420
    },
    {
      "epoch": 2.83003300330033,
      "grad_norm": 1.444225549697876,
      "learning_rate": 2.16996699669967e-05,
      "loss": 0.1297,
      "step": 3430
    },
    {
      "epoch": 2.8382838283828384,
      "grad_norm": 4.250541687011719,
      "learning_rate": 2.161716171617162e-05,
      "loss": 0.1502,
      "step": 3440
    },
    {
      "epoch": 2.8465346534653464,
      "grad_norm": 2.3761115074157715,
      "learning_rate": 2.1534653465346535e-05,
      "loss": 0.1323,
      "step": 3450
    },
    {
      "epoch": 2.854785478547855,
      "grad_norm": 3.205934762954712,
      "learning_rate": 2.1452145214521452e-05,
      "loss": 0.1333,
      "step": 3460
    },
    {
      "epoch": 2.8630363036303628,
      "grad_norm": 2.26595401763916,
      "learning_rate": 2.1369636963696372e-05,
      "loss": 0.1689,
      "step": 3470
    },
    {
      "epoch": 2.871287128712871,
      "grad_norm": 4.397097110748291,
      "learning_rate": 2.128712871287129e-05,
      "loss": 0.1446,
      "step": 3480
    },
    {
      "epoch": 2.8795379537953796,
      "grad_norm": 2.9674084186553955,
      "learning_rate": 2.120462046204621e-05,
      "loss": 0.0906,
      "step": 3490
    },
    {
      "epoch": 2.887788778877888,
      "grad_norm": 2.7180540561676025,
      "learning_rate": 2.1122112211221125e-05,
      "loss": 0.1446,
      "step": 3500
    },
    {
      "epoch": 2.896039603960396,
      "grad_norm": 2.0226433277130127,
      "learning_rate": 2.103960396039604e-05,
      "loss": 0.1447,
      "step": 3510
    },
    {
      "epoch": 2.9042904290429044,
      "grad_norm": 2.3331546783447266,
      "learning_rate": 2.0957095709570958e-05,
      "loss": 0.1532,
      "step": 3520
    },
    {
      "epoch": 2.9125412541254123,
      "grad_norm": 1.5282928943634033,
      "learning_rate": 2.0874587458745874e-05,
      "loss": 0.1154,
      "step": 3530
    },
    {
      "epoch": 2.9207920792079207,
      "grad_norm": 3.0285604000091553,
      "learning_rate": 2.079207920792079e-05,
      "loss": 0.1477,
      "step": 3540
    },
    {
      "epoch": 2.929042904290429,
      "grad_norm": 2.4108455181121826,
      "learning_rate": 2.070957095709571e-05,
      "loss": 0.1339,
      "step": 3550
    },
    {
      "epoch": 2.9372937293729375,
      "grad_norm": 4.072669982910156,
      "learning_rate": 2.0627062706270627e-05,
      "loss": 0.1118,
      "step": 3560
    },
    {
      "epoch": 2.9455445544554455,
      "grad_norm": 2.9755382537841797,
      "learning_rate": 2.0544554455445544e-05,
      "loss": 0.1152,
      "step": 3570
    },
    {
      "epoch": 2.953795379537954,
      "grad_norm": 2.3884246349334717,
      "learning_rate": 2.0462046204620464e-05,
      "loss": 0.1264,
      "step": 3580
    },
    {
      "epoch": 2.962046204620462,
      "grad_norm": 3.5232040882110596,
      "learning_rate": 2.037953795379538e-05,
      "loss": 0.1082,
      "step": 3590
    },
    {
      "epoch": 2.9702970297029703,
      "grad_norm": 3.4299421310424805,
      "learning_rate": 2.02970297029703e-05,
      "loss": 0.141,
      "step": 3600
    },
    {
      "epoch": 2.9785478547854787,
      "grad_norm": 1.455419898033142,
      "learning_rate": 2.0214521452145217e-05,
      "loss": 0.107,
      "step": 3610
    },
    {
      "epoch": 2.9867986798679866,
      "grad_norm": 2.8577568531036377,
      "learning_rate": 2.0132013201320133e-05,
      "loss": 0.1338,
      "step": 3620
    },
    {
      "epoch": 2.995049504950495,
      "grad_norm": 2.8763022422790527,
      "learning_rate": 2.0049504950495053e-05,
      "loss": 0.1066,
      "step": 3630
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.20474353432655334,
      "eval_runtime": 37.4135,
      "eval_samples_per_second": 64.789,
      "eval_steps_per_second": 8.099,
      "step": 3636
    },
    {
      "epoch": 3.0033003300330035,
      "grad_norm": 1.9834173917770386,
      "learning_rate": 1.996699669966997e-05,
      "loss": 0.1172,
      "step": 3640
    },
    {
      "epoch": 3.0115511551155114,
      "grad_norm": 2.9433114528656006,
      "learning_rate": 1.9884488448844886e-05,
      "loss": 0.1123,
      "step": 3650
    },
    {
      "epoch": 3.01980198019802,
      "grad_norm": 2.764389991760254,
      "learning_rate": 1.9801980198019803e-05,
      "loss": 0.07,
      "step": 3660
    },
    {
      "epoch": 3.0280528052805282,
      "grad_norm": 2.924913167953491,
      "learning_rate": 1.971947194719472e-05,
      "loss": 0.0931,
      "step": 3670
    },
    {
      "epoch": 3.036303630363036,
      "grad_norm": 3.3413448333740234,
      "learning_rate": 1.9636963696369636e-05,
      "loss": 0.0924,
      "step": 3680
    },
    {
      "epoch": 3.0445544554455446,
      "grad_norm": 2.365138530731201,
      "learning_rate": 1.9554455445544556e-05,
      "loss": 0.0855,
      "step": 3690
    },
    {
      "epoch": 3.052805280528053,
      "grad_norm": 3.3918683528900146,
      "learning_rate": 1.9471947194719472e-05,
      "loss": 0.1154,
      "step": 3700
    },
    {
      "epoch": 3.061056105610561,
      "grad_norm": 3.571995735168457,
      "learning_rate": 1.938943894389439e-05,
      "loss": 0.0999,
      "step": 3710
    },
    {
      "epoch": 3.0693069306930694,
      "grad_norm": 1.6150926351547241,
      "learning_rate": 1.930693069306931e-05,
      "loss": 0.0826,
      "step": 3720
    },
    {
      "epoch": 3.0775577557755778,
      "grad_norm": 1.1955134868621826,
      "learning_rate": 1.9224422442244225e-05,
      "loss": 0.0802,
      "step": 3730
    },
    {
      "epoch": 3.0858085808580857,
      "grad_norm": 2.4064764976501465,
      "learning_rate": 1.9141914191419145e-05,
      "loss": 0.1042,
      "step": 3740
    },
    {
      "epoch": 3.094059405940594,
      "grad_norm": 2.0983798503875732,
      "learning_rate": 1.905940594059406e-05,
      "loss": 0.0888,
      "step": 3750
    },
    {
      "epoch": 3.102310231023102,
      "grad_norm": 2.347238302230835,
      "learning_rate": 1.8976897689768978e-05,
      "loss": 0.0801,
      "step": 3760
    },
    {
      "epoch": 3.1105610561056105,
      "grad_norm": 1.8727282285690308,
      "learning_rate": 1.8894389438943898e-05,
      "loss": 0.0757,
      "step": 3770
    },
    {
      "epoch": 3.118811881188119,
      "grad_norm": 5.944201946258545,
      "learning_rate": 1.8811881188118814e-05,
      "loss": 0.0849,
      "step": 3780
    },
    {
      "epoch": 3.127062706270627,
      "grad_norm": 6.412418842315674,
      "learning_rate": 1.872937293729373e-05,
      "loss": 0.0874,
      "step": 3790
    },
    {
      "epoch": 3.1353135313531353,
      "grad_norm": 4.164154529571533,
      "learning_rate": 1.8646864686468647e-05,
      "loss": 0.0876,
      "step": 3800
    },
    {
      "epoch": 3.1435643564356437,
      "grad_norm": 2.010601758956909,
      "learning_rate": 1.8564356435643564e-05,
      "loss": 0.1015,
      "step": 3810
    },
    {
      "epoch": 3.1518151815181517,
      "grad_norm": 3.36997127532959,
      "learning_rate": 1.848184818481848e-05,
      "loss": 0.0858,
      "step": 3820
    },
    {
      "epoch": 3.16006600660066,
      "grad_norm": 2.77835750579834,
      "learning_rate": 1.83993399339934e-05,
      "loss": 0.1054,
      "step": 3830
    },
    {
      "epoch": 3.1683168316831685,
      "grad_norm": 2.1661088466644287,
      "learning_rate": 1.8316831683168317e-05,
      "loss": 0.0869,
      "step": 3840
    },
    {
      "epoch": 3.1765676567656764,
      "grad_norm": 2.0916056632995605,
      "learning_rate": 1.8234323432343233e-05,
      "loss": 0.0777,
      "step": 3850
    },
    {
      "epoch": 3.184818481848185,
      "grad_norm": 1.5685136318206787,
      "learning_rate": 1.8151815181518153e-05,
      "loss": 0.0885,
      "step": 3860
    },
    {
      "epoch": 3.1930693069306932,
      "grad_norm": 4.039893627166748,
      "learning_rate": 1.806930693069307e-05,
      "loss": 0.1074,
      "step": 3870
    },
    {
      "epoch": 3.201320132013201,
      "grad_norm": 2.299074649810791,
      "learning_rate": 1.798679867986799e-05,
      "loss": 0.0852,
      "step": 3880
    },
    {
      "epoch": 3.2095709570957096,
      "grad_norm": 3.2551560401916504,
      "learning_rate": 1.7904290429042906e-05,
      "loss": 0.0988,
      "step": 3890
    },
    {
      "epoch": 3.217821782178218,
      "grad_norm": 2.704773187637329,
      "learning_rate": 1.7821782178217823e-05,
      "loss": 0.0706,
      "step": 3900
    },
    {
      "epoch": 3.226072607260726,
      "grad_norm": 2.0379786491394043,
      "learning_rate": 1.7739273927392743e-05,
      "loss": 0.0935,
      "step": 3910
    },
    {
      "epoch": 3.2343234323432344,
      "grad_norm": 2.987525224685669,
      "learning_rate": 1.765676567656766e-05,
      "loss": 0.095,
      "step": 3920
    },
    {
      "epoch": 3.2425742574257423,
      "grad_norm": 2.5137882232666016,
      "learning_rate": 1.7574257425742576e-05,
      "loss": 0.0708,
      "step": 3930
    },
    {
      "epoch": 3.2508250825082508,
      "grad_norm": 5.582239627838135,
      "learning_rate": 1.7491749174917492e-05,
      "loss": 0.0714,
      "step": 3940
    },
    {
      "epoch": 3.259075907590759,
      "grad_norm": 5.325028419494629,
      "learning_rate": 1.740924092409241e-05,
      "loss": 0.0607,
      "step": 3950
    },
    {
      "epoch": 3.2673267326732676,
      "grad_norm": 2.2846765518188477,
      "learning_rate": 1.7326732673267325e-05,
      "loss": 0.1123,
      "step": 3960
    },
    {
      "epoch": 3.2755775577557755,
      "grad_norm": 2.60915470123291,
      "learning_rate": 1.7244224422442245e-05,
      "loss": 0.0941,
      "step": 3970
    },
    {
      "epoch": 3.283828382838284,
      "grad_norm": 3.515720844268799,
      "learning_rate": 1.716171617161716e-05,
      "loss": 0.0872,
      "step": 3980
    },
    {
      "epoch": 3.292079207920792,
      "grad_norm": 4.783076763153076,
      "learning_rate": 1.707920792079208e-05,
      "loss": 0.0907,
      "step": 3990
    },
    {
      "epoch": 3.3003300330033003,
      "grad_norm": 2.1059391498565674,
      "learning_rate": 1.6996699669966998e-05,
      "loss": 0.1276,
      "step": 4000
    },
    {
      "epoch": 3.3085808580858087,
      "grad_norm": 1.2232069969177246,
      "learning_rate": 1.6914191419141915e-05,
      "loss": 0.0769,
      "step": 4010
    },
    {
      "epoch": 3.3168316831683167,
      "grad_norm": 1.4734677076339722,
      "learning_rate": 1.6831683168316834e-05,
      "loss": 0.0829,
      "step": 4020
    },
    {
      "epoch": 3.325082508250825,
      "grad_norm": 3.5358505249023438,
      "learning_rate": 1.674917491749175e-05,
      "loss": 0.0723,
      "step": 4030
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 4.613073348999023,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.1053,
      "step": 4040
    },
    {
      "epoch": 3.3415841584158414,
      "grad_norm": 3.983272075653076,
      "learning_rate": 1.6584158415841587e-05,
      "loss": 0.1173,
      "step": 4050
    },
    {
      "epoch": 3.34983498349835,
      "grad_norm": 1.749751091003418,
      "learning_rate": 1.6501650165016504e-05,
      "loss": 0.0801,
      "step": 4060
    },
    {
      "epoch": 3.3580858085808583,
      "grad_norm": 1.9190921783447266,
      "learning_rate": 1.641914191419142e-05,
      "loss": 0.0743,
      "step": 4070
    },
    {
      "epoch": 3.366336633663366,
      "grad_norm": 3.7644479274749756,
      "learning_rate": 1.6336633663366337e-05,
      "loss": 0.1079,
      "step": 4080
    },
    {
      "epoch": 3.3745874587458746,
      "grad_norm": 4.3179121017456055,
      "learning_rate": 1.6254125412541253e-05,
      "loss": 0.0801,
      "step": 4090
    },
    {
      "epoch": 3.382838283828383,
      "grad_norm": 2.8934178352355957,
      "learning_rate": 1.617161716171617e-05,
      "loss": 0.0878,
      "step": 4100
    },
    {
      "epoch": 3.391089108910891,
      "grad_norm": 2.267192840576172,
      "learning_rate": 1.608910891089109e-05,
      "loss": 0.1032,
      "step": 4110
    },
    {
      "epoch": 3.3993399339933994,
      "grad_norm": 2.9400064945220947,
      "learning_rate": 1.6006600660066006e-05,
      "loss": 0.109,
      "step": 4120
    },
    {
      "epoch": 3.407590759075908,
      "grad_norm": 2.7797183990478516,
      "learning_rate": 1.5924092409240926e-05,
      "loss": 0.0868,
      "step": 4130
    },
    {
      "epoch": 3.4158415841584158,
      "grad_norm": 3.011042594909668,
      "learning_rate": 1.5841584158415843e-05,
      "loss": 0.1095,
      "step": 4140
    },
    {
      "epoch": 3.424092409240924,
      "grad_norm": 3.4588804244995117,
      "learning_rate": 1.575907590759076e-05,
      "loss": 0.0975,
      "step": 4150
    },
    {
      "epoch": 3.432343234323432,
      "grad_norm": 2.454103469848633,
      "learning_rate": 1.567656765676568e-05,
      "loss": 0.0905,
      "step": 4160
    },
    {
      "epoch": 3.4405940594059405,
      "grad_norm": 3.6119489669799805,
      "learning_rate": 1.5594059405940596e-05,
      "loss": 0.0783,
      "step": 4170
    },
    {
      "epoch": 3.448844884488449,
      "grad_norm": 2.4791831970214844,
      "learning_rate": 1.5511551155115512e-05,
      "loss": 0.091,
      "step": 4180
    },
    {
      "epoch": 3.457095709570957,
      "grad_norm": 1.9328689575195312,
      "learning_rate": 1.5429042904290432e-05,
      "loss": 0.0709,
      "step": 4190
    },
    {
      "epoch": 3.4653465346534653,
      "grad_norm": 2.0310983657836914,
      "learning_rate": 1.534653465346535e-05,
      "loss": 0.0749,
      "step": 4200
    },
    {
      "epoch": 3.4735973597359737,
      "grad_norm": 2.7952072620391846,
      "learning_rate": 1.5264026402640265e-05,
      "loss": 0.0841,
      "step": 4210
    },
    {
      "epoch": 3.4818481848184817,
      "grad_norm": 2.544964551925659,
      "learning_rate": 1.5181518151815183e-05,
      "loss": 0.0816,
      "step": 4220
    },
    {
      "epoch": 3.49009900990099,
      "grad_norm": 1.6476186513900757,
      "learning_rate": 1.50990099009901e-05,
      "loss": 0.0862,
      "step": 4230
    },
    {
      "epoch": 3.4983498349834985,
      "grad_norm": 2.9331560134887695,
      "learning_rate": 1.5016501650165018e-05,
      "loss": 0.088,
      "step": 4240
    },
    {
      "epoch": 3.5066006600660065,
      "grad_norm": 2.8864221572875977,
      "learning_rate": 1.4933993399339935e-05,
      "loss": 0.081,
      "step": 4250
    },
    {
      "epoch": 3.514851485148515,
      "grad_norm": 2.9926538467407227,
      "learning_rate": 1.4851485148514851e-05,
      "loss": 0.0966,
      "step": 4260
    },
    {
      "epoch": 3.523102310231023,
      "grad_norm": 1.8176262378692627,
      "learning_rate": 1.4768976897689771e-05,
      "loss": 0.0926,
      "step": 4270
    },
    {
      "epoch": 3.5313531353135312,
      "grad_norm": 2.591346263885498,
      "learning_rate": 1.4686468646864688e-05,
      "loss": 0.0749,
      "step": 4280
    },
    {
      "epoch": 3.5396039603960396,
      "grad_norm": 1.5653599500656128,
      "learning_rate": 1.4603960396039604e-05,
      "loss": 0.0686,
      "step": 4290
    },
    {
      "epoch": 3.547854785478548,
      "grad_norm": 3.6865687370300293,
      "learning_rate": 1.4521452145214522e-05,
      "loss": 0.0926,
      "step": 4300
    },
    {
      "epoch": 3.556105610561056,
      "grad_norm": 2.3767240047454834,
      "learning_rate": 1.4438943894389439e-05,
      "loss": 0.0584,
      "step": 4310
    },
    {
      "epoch": 3.5643564356435644,
      "grad_norm": 2.2907392978668213,
      "learning_rate": 1.4356435643564355e-05,
      "loss": 0.089,
      "step": 4320
    },
    {
      "epoch": 3.5726072607260724,
      "grad_norm": 2.4958269596099854,
      "learning_rate": 1.4273927392739275e-05,
      "loss": 0.1024,
      "step": 4330
    },
    {
      "epoch": 3.580858085808581,
      "grad_norm": 6.683300495147705,
      "learning_rate": 1.4191419141914192e-05,
      "loss": 0.0896,
      "step": 4340
    },
    {
      "epoch": 3.589108910891089,
      "grad_norm": 2.211655616760254,
      "learning_rate": 1.4108910891089108e-05,
      "loss": 0.0673,
      "step": 4350
    },
    {
      "epoch": 3.5973597359735976,
      "grad_norm": 2.7599916458129883,
      "learning_rate": 1.4026402640264028e-05,
      "loss": 0.0728,
      "step": 4360
    },
    {
      "epoch": 3.6056105610561056,
      "grad_norm": 4.511667251586914,
      "learning_rate": 1.3943894389438945e-05,
      "loss": 0.0872,
      "step": 4370
    },
    {
      "epoch": 3.613861386138614,
      "grad_norm": 3.1901090145111084,
      "learning_rate": 1.3861386138613863e-05,
      "loss": 0.0944,
      "step": 4380
    },
    {
      "epoch": 3.622112211221122,
      "grad_norm": 1.9562160968780518,
      "learning_rate": 1.377887788778878e-05,
      "loss": 0.0729,
      "step": 4390
    },
    {
      "epoch": 3.6303630363036303,
      "grad_norm": 2.4038407802581787,
      "learning_rate": 1.3696369636963696e-05,
      "loss": 0.0749,
      "step": 4400
    },
    {
      "epoch": 3.6386138613861387,
      "grad_norm": 2.4879465103149414,
      "learning_rate": 1.3613861386138616e-05,
      "loss": 0.0827,
      "step": 4410
    },
    {
      "epoch": 3.6468646864686467,
      "grad_norm": 5.113592147827148,
      "learning_rate": 1.3531353135313532e-05,
      "loss": 0.0982,
      "step": 4420
    },
    {
      "epoch": 3.655115511551155,
      "grad_norm": 2.387761354446411,
      "learning_rate": 1.3448844884488449e-05,
      "loss": 0.0822,
      "step": 4430
    },
    {
      "epoch": 3.6633663366336635,
      "grad_norm": 3.6801202297210693,
      "learning_rate": 1.3366336633663367e-05,
      "loss": 0.0987,
      "step": 4440
    },
    {
      "epoch": 3.6716171617161715,
      "grad_norm": 2.50948166847229,
      "learning_rate": 1.3283828382838284e-05,
      "loss": 0.1112,
      "step": 4450
    },
    {
      "epoch": 3.67986798679868,
      "grad_norm": 3.0015642642974854,
      "learning_rate": 1.32013201320132e-05,
      "loss": 0.0788,
      "step": 4460
    },
    {
      "epoch": 3.6881188118811883,
      "grad_norm": 2.734482526779175,
      "learning_rate": 1.311881188118812e-05,
      "loss": 0.0854,
      "step": 4470
    },
    {
      "epoch": 3.6963696369636962,
      "grad_norm": 3.5517168045043945,
      "learning_rate": 1.3036303630363036e-05,
      "loss": 0.0883,
      "step": 4480
    },
    {
      "epoch": 3.7046204620462047,
      "grad_norm": 1.9006497859954834,
      "learning_rate": 1.2953795379537956e-05,
      "loss": 0.1008,
      "step": 4490
    },
    {
      "epoch": 3.7128712871287126,
      "grad_norm": 4.09365177154541,
      "learning_rate": 1.2871287128712873e-05,
      "loss": 0.1017,
      "step": 4500
    },
    {
      "epoch": 3.721122112211221,
      "grad_norm": 3.9130606651306152,
      "learning_rate": 1.278877887788779e-05,
      "loss": 0.0828,
      "step": 4510
    },
    {
      "epoch": 3.7293729372937294,
      "grad_norm": 1.7760028839111328,
      "learning_rate": 1.2706270627062708e-05,
      "loss": 0.0823,
      "step": 4520
    },
    {
      "epoch": 3.737623762376238,
      "grad_norm": 3.26178240776062,
      "learning_rate": 1.2623762376237624e-05,
      "loss": 0.0772,
      "step": 4530
    },
    {
      "epoch": 3.745874587458746,
      "grad_norm": 3.2970376014709473,
      "learning_rate": 1.254125412541254e-05,
      "loss": 0.11,
      "step": 4540
    },
    {
      "epoch": 3.754125412541254,
      "grad_norm": 1.9329053163528442,
      "learning_rate": 1.2458745874587459e-05,
      "loss": 0.0758,
      "step": 4550
    },
    {
      "epoch": 3.762376237623762,
      "grad_norm": 2.9214649200439453,
      "learning_rate": 1.2376237623762377e-05,
      "loss": 0.1293,
      "step": 4560
    },
    {
      "epoch": 3.7706270627062706,
      "grad_norm": 1.3297204971313477,
      "learning_rate": 1.2293729372937295e-05,
      "loss": 0.09,
      "step": 4570
    },
    {
      "epoch": 3.778877887788779,
      "grad_norm": 2.5972158908843994,
      "learning_rate": 1.2211221122112212e-05,
      "loss": 0.0709,
      "step": 4580
    },
    {
      "epoch": 3.7871287128712874,
      "grad_norm": 1.2122468948364258,
      "learning_rate": 1.2128712871287128e-05,
      "loss": 0.0695,
      "step": 4590
    },
    {
      "epoch": 3.7953795379537953,
      "grad_norm": 2.8226089477539062,
      "learning_rate": 1.2046204620462047e-05,
      "loss": 0.0843,
      "step": 4600
    },
    {
      "epoch": 3.8036303630363038,
      "grad_norm": 3.7365317344665527,
      "learning_rate": 1.1963696369636965e-05,
      "loss": 0.0544,
      "step": 4610
    },
    {
      "epoch": 3.8118811881188117,
      "grad_norm": 3.6812562942504883,
      "learning_rate": 1.1881188118811881e-05,
      "loss": 0.0802,
      "step": 4620
    },
    {
      "epoch": 3.82013201320132,
      "grad_norm": 3.209902286529541,
      "learning_rate": 1.17986798679868e-05,
      "loss": 0.1005,
      "step": 4630
    },
    {
      "epoch": 3.8283828382838285,
      "grad_norm": 2.5149779319763184,
      "learning_rate": 1.1716171617161718e-05,
      "loss": 0.1086,
      "step": 4640
    },
    {
      "epoch": 3.8366336633663365,
      "grad_norm": 2.9455153942108154,
      "learning_rate": 1.1633663366336634e-05,
      "loss": 0.0956,
      "step": 4650
    },
    {
      "epoch": 3.844884488448845,
      "grad_norm": 3.758772134780884,
      "learning_rate": 1.155115511551155e-05,
      "loss": 0.117,
      "step": 4660
    },
    {
      "epoch": 3.8531353135313533,
      "grad_norm": 1.3312288522720337,
      "learning_rate": 1.1468646864686469e-05,
      "loss": 0.0967,
      "step": 4670
    },
    {
      "epoch": 3.8613861386138613,
      "grad_norm": 2.095182180404663,
      "learning_rate": 1.1386138613861387e-05,
      "loss": 0.0692,
      "step": 4680
    },
    {
      "epoch": 3.8696369636963697,
      "grad_norm": 3.279209613800049,
      "learning_rate": 1.1303630363036304e-05,
      "loss": 0.0901,
      "step": 4690
    },
    {
      "epoch": 3.877887788778878,
      "grad_norm": 1.6851192712783813,
      "learning_rate": 1.1221122112211222e-05,
      "loss": 0.0894,
      "step": 4700
    },
    {
      "epoch": 3.886138613861386,
      "grad_norm": 2.3935205936431885,
      "learning_rate": 1.113861386138614e-05,
      "loss": 0.0947,
      "step": 4710
    },
    {
      "epoch": 3.8943894389438944,
      "grad_norm": 3.525268793106079,
      "learning_rate": 1.1056105610561057e-05,
      "loss": 0.0825,
      "step": 4720
    },
    {
      "epoch": 3.9026402640264024,
      "grad_norm": 3.3328816890716553,
      "learning_rate": 1.0973597359735973e-05,
      "loss": 0.0686,
      "step": 4730
    },
    {
      "epoch": 3.910891089108911,
      "grad_norm": 3.684703826904297,
      "learning_rate": 1.0891089108910891e-05,
      "loss": 0.0949,
      "step": 4740
    },
    {
      "epoch": 3.919141914191419,
      "grad_norm": 1.5742673873901367,
      "learning_rate": 1.080858085808581e-05,
      "loss": 0.0945,
      "step": 4750
    },
    {
      "epoch": 3.9273927392739276,
      "grad_norm": 1.7842122316360474,
      "learning_rate": 1.0726072607260726e-05,
      "loss": 0.081,
      "step": 4760
    },
    {
      "epoch": 3.9356435643564356,
      "grad_norm": 3.3521206378936768,
      "learning_rate": 1.0643564356435644e-05,
      "loss": 0.0802,
      "step": 4770
    },
    {
      "epoch": 3.943894389438944,
      "grad_norm": 3.648653745651245,
      "learning_rate": 1.0561056105610562e-05,
      "loss": 0.0759,
      "step": 4780
    },
    {
      "epoch": 3.952145214521452,
      "grad_norm": 2.6751339435577393,
      "learning_rate": 1.0478547854785479e-05,
      "loss": 0.1106,
      "step": 4790
    },
    {
      "epoch": 3.9603960396039604,
      "grad_norm": 2.0429396629333496,
      "learning_rate": 1.0396039603960395e-05,
      "loss": 0.0804,
      "step": 4800
    },
    {
      "epoch": 3.9686468646864688,
      "grad_norm": 0.9025760889053345,
      "learning_rate": 1.0313531353135314e-05,
      "loss": 0.0731,
      "step": 4810
    },
    {
      "epoch": 3.976897689768977,
      "grad_norm": 2.182894468307495,
      "learning_rate": 1.0231023102310232e-05,
      "loss": 0.0695,
      "step": 4820
    },
    {
      "epoch": 3.985148514851485,
      "grad_norm": 1.8086268901824951,
      "learning_rate": 1.014851485148515e-05,
      "loss": 0.0825,
      "step": 4830
    },
    {
      "epoch": 3.9933993399339935,
      "grad_norm": 2.0810604095458984,
      "learning_rate": 1.0066006600660067e-05,
      "loss": 0.0726,
      "step": 4840
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.21562863886356354,
      "eval_runtime": 24.0141,
      "eval_samples_per_second": 100.941,
      "eval_steps_per_second": 12.618,
      "step": 4848
    },
    {
      "epoch": 4.0016501650165015,
      "grad_norm": 1.845665454864502,
      "learning_rate": 9.983498349834985e-06,
      "loss": 0.0775,
      "step": 4850
    },
    {
      "epoch": 4.00990099009901,
      "grad_norm": 1.3786306381225586,
      "learning_rate": 9.900990099009901e-06,
      "loss": 0.0531,
      "step": 4860
    },
    {
      "epoch": 4.018151815181518,
      "grad_norm": 10.92136001586914,
      "learning_rate": 9.818481848184818e-06,
      "loss": 0.0599,
      "step": 4870
    },
    {
      "epoch": 4.026402640264027,
      "grad_norm": 1.3116639852523804,
      "learning_rate": 9.735973597359736e-06,
      "loss": 0.0573,
      "step": 4880
    },
    {
      "epoch": 4.034653465346534,
      "grad_norm": 2.2972514629364014,
      "learning_rate": 9.653465346534654e-06,
      "loss": 0.0391,
      "step": 4890
    },
    {
      "epoch": 4.042904290429043,
      "grad_norm": 3.2835001945495605,
      "learning_rate": 9.570957095709572e-06,
      "loss": 0.0712,
      "step": 4900
    },
    {
      "epoch": 4.051155115511551,
      "grad_norm": 1.438025712966919,
      "learning_rate": 9.488448844884489e-06,
      "loss": 0.0381,
      "step": 4910
    },
    {
      "epoch": 4.0594059405940595,
      "grad_norm": 3.6656336784362793,
      "learning_rate": 9.405940594059407e-06,
      "loss": 0.0461,
      "step": 4920
    },
    {
      "epoch": 4.067656765676568,
      "grad_norm": 0.9224796891212463,
      "learning_rate": 9.323432343234324e-06,
      "loss": 0.068,
      "step": 4930
    },
    {
      "epoch": 4.075907590759076,
      "grad_norm": 3.56288743019104,
      "learning_rate": 9.24092409240924e-06,
      "loss": 0.068,
      "step": 4940
    },
    {
      "epoch": 4.084158415841584,
      "grad_norm": 1.188374638557434,
      "learning_rate": 9.158415841584158e-06,
      "loss": 0.0553,
      "step": 4950
    },
    {
      "epoch": 4.092409240924092,
      "grad_norm": 2.3086929321289062,
      "learning_rate": 9.075907590759077e-06,
      "loss": 0.0372,
      "step": 4960
    },
    {
      "epoch": 4.100660066006601,
      "grad_norm": 4.481050968170166,
      "learning_rate": 8.993399339933995e-06,
      "loss": 0.0709,
      "step": 4970
    },
    {
      "epoch": 4.108910891089109,
      "grad_norm": 1.9666386842727661,
      "learning_rate": 8.910891089108911e-06,
      "loss": 0.0974,
      "step": 4980
    },
    {
      "epoch": 4.117161716171617,
      "grad_norm": 2.81447696685791,
      "learning_rate": 8.82838283828383e-06,
      "loss": 0.0527,
      "step": 4990
    },
    {
      "epoch": 4.125412541254126,
      "grad_norm": 1.9968966245651245,
      "learning_rate": 8.745874587458746e-06,
      "loss": 0.045,
      "step": 5000
    },
    {
      "epoch": 4.133663366336633,
      "grad_norm": 2.2512190341949463,
      "learning_rate": 8.663366336633663e-06,
      "loss": 0.0491,
      "step": 5010
    },
    {
      "epoch": 4.141914191419142,
      "grad_norm": 2.332753896713257,
      "learning_rate": 8.58085808580858e-06,
      "loss": 0.0521,
      "step": 5020
    },
    {
      "epoch": 4.15016501650165,
      "grad_norm": 0.9054931998252869,
      "learning_rate": 8.498349834983499e-06,
      "loss": 0.0596,
      "step": 5030
    },
    {
      "epoch": 4.158415841584159,
      "grad_norm": 1.2800143957138062,
      "learning_rate": 8.415841584158417e-06,
      "loss": 0.0487,
      "step": 5040
    },
    {
      "epoch": 4.166666666666667,
      "grad_norm": 4.700650691986084,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.0623,
      "step": 5050
    },
    {
      "epoch": 4.174917491749175,
      "grad_norm": 2.0151100158691406,
      "learning_rate": 8.250825082508252e-06,
      "loss": 0.0533,
      "step": 5060
    },
    {
      "epoch": 4.183168316831683,
      "grad_norm": 3.6483378410339355,
      "learning_rate": 8.168316831683168e-06,
      "loss": 0.0469,
      "step": 5070
    },
    {
      "epoch": 4.191419141914191,
      "grad_norm": 0.8561859130859375,
      "learning_rate": 8.085808580858085e-06,
      "loss": 0.0457,
      "step": 5080
    },
    {
      "epoch": 4.1996699669967,
      "grad_norm": 4.04488468170166,
      "learning_rate": 8.003300330033003e-06,
      "loss": 0.0452,
      "step": 5090
    },
    {
      "epoch": 4.207920792079208,
      "grad_norm": 1.9077223539352417,
      "learning_rate": 7.920792079207921e-06,
      "loss": 0.0707,
      "step": 5100
    },
    {
      "epoch": 4.2161716171617165,
      "grad_norm": 1.5594139099121094,
      "learning_rate": 7.83828382838284e-06,
      "loss": 0.0439,
      "step": 5110
    },
    {
      "epoch": 4.224422442244224,
      "grad_norm": 1.2080857753753662,
      "learning_rate": 7.755775577557756e-06,
      "loss": 0.0356,
      "step": 5120
    },
    {
      "epoch": 4.232673267326732,
      "grad_norm": 1.0205600261688232,
      "learning_rate": 7.673267326732674e-06,
      "loss": 0.0518,
      "step": 5130
    },
    {
      "epoch": 4.240924092409241,
      "grad_norm": 1.7603646516799927,
      "learning_rate": 7.590759075907592e-06,
      "loss": 0.0497,
      "step": 5140
    },
    {
      "epoch": 4.249174917491749,
      "grad_norm": 2.0905039310455322,
      "learning_rate": 7.508250825082509e-06,
      "loss": 0.0641,
      "step": 5150
    },
    {
      "epoch": 4.257425742574258,
      "grad_norm": 1.6780502796173096,
      "learning_rate": 7.4257425742574256e-06,
      "loss": 0.0396,
      "step": 5160
    },
    {
      "epoch": 4.265676567656766,
      "grad_norm": 4.540447235107422,
      "learning_rate": 7.343234323432344e-06,
      "loss": 0.0663,
      "step": 5170
    },
    {
      "epoch": 4.273927392739274,
      "grad_norm": 5.583609580993652,
      "learning_rate": 7.260726072607261e-06,
      "loss": 0.0705,
      "step": 5180
    },
    {
      "epoch": 4.282178217821782,
      "grad_norm": 0.8500692844390869,
      "learning_rate": 7.178217821782178e-06,
      "loss": 0.0548,
      "step": 5190
    },
    {
      "epoch": 4.29042904290429,
      "grad_norm": 1.8330798149108887,
      "learning_rate": 7.095709570957096e-06,
      "loss": 0.0591,
      "step": 5200
    },
    {
      "epoch": 4.298679867986799,
      "grad_norm": 5.684508323669434,
      "learning_rate": 7.013201320132014e-06,
      "loss": 0.0556,
      "step": 5210
    },
    {
      "epoch": 4.306930693069307,
      "grad_norm": 5.084211826324463,
      "learning_rate": 6.9306930693069314e-06,
      "loss": 0.0716,
      "step": 5220
    },
    {
      "epoch": 4.315181518151816,
      "grad_norm": 3.6105713844299316,
      "learning_rate": 6.848184818481848e-06,
      "loss": 0.057,
      "step": 5230
    },
    {
      "epoch": 4.323432343234323,
      "grad_norm": 1.2913073301315308,
      "learning_rate": 6.765676567656766e-06,
      "loss": 0.0472,
      "step": 5240
    },
    {
      "epoch": 4.3316831683168315,
      "grad_norm": 1.2310881614685059,
      "learning_rate": 6.6831683168316835e-06,
      "loss": 0.069,
      "step": 5250
    },
    {
      "epoch": 4.33993399339934,
      "grad_norm": 1.717740535736084,
      "learning_rate": 6.6006600660066e-06,
      "loss": 0.0584,
      "step": 5260
    },
    {
      "epoch": 4.348184818481848,
      "grad_norm": 5.991613388061523,
      "learning_rate": 6.518151815181518e-06,
      "loss": 0.062,
      "step": 5270
    },
    {
      "epoch": 4.356435643564357,
      "grad_norm": 1.0959802865982056,
      "learning_rate": 6.4356435643564364e-06,
      "loss": 0.0489,
      "step": 5280
    },
    {
      "epoch": 4.364686468646864,
      "grad_norm": 2.579653263092041,
      "learning_rate": 6.353135313531354e-06,
      "loss": 0.0722,
      "step": 5290
    },
    {
      "epoch": 4.372937293729373,
      "grad_norm": 3.747560501098633,
      "learning_rate": 6.27062706270627e-06,
      "loss": 0.0474,
      "step": 5300
    },
    {
      "epoch": 4.381188118811881,
      "grad_norm": 2.435551404953003,
      "learning_rate": 6.1881188118811885e-06,
      "loss": 0.048,
      "step": 5310
    },
    {
      "epoch": 4.3894389438943895,
      "grad_norm": 2.0098259449005127,
      "learning_rate": 6.105610561056106e-06,
      "loss": 0.0452,
      "step": 5320
    },
    {
      "epoch": 4.397689768976898,
      "grad_norm": 2.4657528400421143,
      "learning_rate": 6.023102310231023e-06,
      "loss": 0.059,
      "step": 5330
    },
    {
      "epoch": 4.405940594059406,
      "grad_norm": 2.7771263122558594,
      "learning_rate": 5.940594059405941e-06,
      "loss": 0.0477,
      "step": 5340
    },
    {
      "epoch": 4.414191419141914,
      "grad_norm": 2.2782793045043945,
      "learning_rate": 5.858085808580859e-06,
      "loss": 0.0521,
      "step": 5350
    },
    {
      "epoch": 4.422442244224422,
      "grad_norm": 1.95285165309906,
      "learning_rate": 5.775577557755775e-06,
      "loss": 0.0654,
      "step": 5360
    },
    {
      "epoch": 4.430693069306931,
      "grad_norm": 2.779146909713745,
      "learning_rate": 5.6930693069306936e-06,
      "loss": 0.0595,
      "step": 5370
    },
    {
      "epoch": 4.438943894389439,
      "grad_norm": 7.607802391052246,
      "learning_rate": 5.610561056105611e-06,
      "loss": 0.0613,
      "step": 5380
    },
    {
      "epoch": 4.447194719471947,
      "grad_norm": 3.3488235473632812,
      "learning_rate": 5.528052805280528e-06,
      "loss": 0.0394,
      "step": 5390
    },
    {
      "epoch": 4.455445544554456,
      "grad_norm": 4.8118767738342285,
      "learning_rate": 5.445544554455446e-06,
      "loss": 0.0613,
      "step": 5400
    },
    {
      "epoch": 4.463696369636963,
      "grad_norm": 2.938495635986328,
      "learning_rate": 5.363036303630363e-06,
      "loss": 0.0615,
      "step": 5410
    },
    {
      "epoch": 4.471947194719472,
      "grad_norm": 2.3191118240356445,
      "learning_rate": 5.280528052805281e-06,
      "loss": 0.0485,
      "step": 5420
    },
    {
      "epoch": 4.48019801980198,
      "grad_norm": 2.338127374649048,
      "learning_rate": 5.198019801980198e-06,
      "loss": 0.0466,
      "step": 5430
    },
    {
      "epoch": 4.488448844884489,
      "grad_norm": 1.2909252643585205,
      "learning_rate": 5.115511551155116e-06,
      "loss": 0.0521,
      "step": 5440
    },
    {
      "epoch": 4.496699669966997,
      "grad_norm": 2.558471202850342,
      "learning_rate": 5.033003300330033e-06,
      "loss": 0.0386,
      "step": 5450
    },
    {
      "epoch": 4.5049504950495045,
      "grad_norm": 2.3158724308013916,
      "learning_rate": 4.950495049504951e-06,
      "loss": 0.0566,
      "step": 5460
    },
    {
      "epoch": 4.513201320132013,
      "grad_norm": 5.657148838043213,
      "learning_rate": 4.867986798679868e-06,
      "loss": 0.0422,
      "step": 5470
    },
    {
      "epoch": 4.521452145214521,
      "grad_norm": 3.1400222778320312,
      "learning_rate": 4.785478547854786e-06,
      "loss": 0.0369,
      "step": 5480
    },
    {
      "epoch": 4.52970297029703,
      "grad_norm": 2.6595447063446045,
      "learning_rate": 4.702970297029704e-06,
      "loss": 0.0662,
      "step": 5490
    },
    {
      "epoch": 4.537953795379538,
      "grad_norm": 2.300672769546509,
      "learning_rate": 4.62046204620462e-06,
      "loss": 0.0747,
      "step": 5500
    },
    {
      "epoch": 4.5462046204620465,
      "grad_norm": 2.203240394592285,
      "learning_rate": 4.537953795379538e-06,
      "loss": 0.0602,
      "step": 5510
    },
    {
      "epoch": 4.554455445544555,
      "grad_norm": 2.5963339805603027,
      "learning_rate": 4.455445544554456e-06,
      "loss": 0.0423,
      "step": 5520
    },
    {
      "epoch": 4.5627062706270625,
      "grad_norm": 2.061345100402832,
      "learning_rate": 4.372937293729373e-06,
      "loss": 0.0366,
      "step": 5530
    },
    {
      "epoch": 4.570957095709571,
      "grad_norm": 3.4332799911499023,
      "learning_rate": 4.29042904290429e-06,
      "loss": 0.057,
      "step": 5540
    },
    {
      "epoch": 4.579207920792079,
      "grad_norm": 5.170474529266357,
      "learning_rate": 4.207920792079209e-06,
      "loss": 0.0571,
      "step": 5550
    },
    {
      "epoch": 4.587458745874588,
      "grad_norm": 2.7914249897003174,
      "learning_rate": 4.125412541254126e-06,
      "loss": 0.0444,
      "step": 5560
    },
    {
      "epoch": 4.595709570957096,
      "grad_norm": 6.109589576721191,
      "learning_rate": 4.0429042904290425e-06,
      "loss": 0.0613,
      "step": 5570
    },
    {
      "epoch": 4.603960396039604,
      "grad_norm": 6.117341995239258,
      "learning_rate": 3.960396039603961e-06,
      "loss": 0.0458,
      "step": 5580
    },
    {
      "epoch": 4.612211221122112,
      "grad_norm": 3.9704372882843018,
      "learning_rate": 3.877887788778878e-06,
      "loss": 0.0531,
      "step": 5590
    },
    {
      "epoch": 4.62046204620462,
      "grad_norm": 3.2904672622680664,
      "learning_rate": 3.795379537953796e-06,
      "loss": 0.052,
      "step": 5600
    },
    {
      "epoch": 4.628712871287129,
      "grad_norm": 4.0041632652282715,
      "learning_rate": 3.7128712871287128e-06,
      "loss": 0.0704,
      "step": 5610
    },
    {
      "epoch": 4.636963696369637,
      "grad_norm": 1.2042897939682007,
      "learning_rate": 3.6303630363036306e-06,
      "loss": 0.0685,
      "step": 5620
    },
    {
      "epoch": 4.645214521452145,
      "grad_norm": 4.205050945281982,
      "learning_rate": 3.547854785478548e-06,
      "loss": 0.0662,
      "step": 5630
    },
    {
      "epoch": 4.653465346534653,
      "grad_norm": 3.42069673538208,
      "learning_rate": 3.4653465346534657e-06,
      "loss": 0.0593,
      "step": 5640
    },
    {
      "epoch": 4.661716171617162,
      "grad_norm": 3.4069855213165283,
      "learning_rate": 3.382838283828383e-06,
      "loss": 0.0624,
      "step": 5650
    },
    {
      "epoch": 4.66996699669967,
      "grad_norm": 1.8349632024765015,
      "learning_rate": 3.3003300330033e-06,
      "loss": 0.061,
      "step": 5660
    },
    {
      "epoch": 4.678217821782178,
      "grad_norm": 4.248758792877197,
      "learning_rate": 3.2178217821782182e-06,
      "loss": 0.0624,
      "step": 5670
    },
    {
      "epoch": 4.686468646864687,
      "grad_norm": 2.552629232406616,
      "learning_rate": 3.135313531353135e-06,
      "loss": 0.0478,
      "step": 5680
    },
    {
      "epoch": 4.694719471947195,
      "grad_norm": 6.875523567199707,
      "learning_rate": 3.052805280528053e-06,
      "loss": 0.0406,
      "step": 5690
    },
    {
      "epoch": 4.702970297029703,
      "grad_norm": 1.0444570779800415,
      "learning_rate": 2.9702970297029703e-06,
      "loss": 0.0482,
      "step": 5700
    },
    {
      "epoch": 4.711221122112211,
      "grad_norm": 4.749245643615723,
      "learning_rate": 2.8877887788778877e-06,
      "loss": 0.0656,
      "step": 5710
    },
    {
      "epoch": 4.7194719471947195,
      "grad_norm": 3.0214452743530273,
      "learning_rate": 2.8052805280528055e-06,
      "loss": 0.0385,
      "step": 5720
    },
    {
      "epoch": 4.727722772277228,
      "grad_norm": 3.787503719329834,
      "learning_rate": 2.722772277227723e-06,
      "loss": 0.0666,
      "step": 5730
    },
    {
      "epoch": 4.735973597359736,
      "grad_norm": 2.699537515640259,
      "learning_rate": 2.6402640264026406e-06,
      "loss": 0.058,
      "step": 5740
    },
    {
      "epoch": 4.744224422442244,
      "grad_norm": 3.0886356830596924,
      "learning_rate": 2.557755775577558e-06,
      "loss": 0.0499,
      "step": 5750
    },
    {
      "epoch": 4.752475247524752,
      "grad_norm": 2.008983612060547,
      "learning_rate": 2.4752475247524753e-06,
      "loss": 0.0383,
      "step": 5760
    },
    {
      "epoch": 4.760726072607261,
      "grad_norm": 1.8837347030639648,
      "learning_rate": 2.392739273927393e-06,
      "loss": 0.0506,
      "step": 5770
    },
    {
      "epoch": 4.768976897689769,
      "grad_norm": 5.264936923980713,
      "learning_rate": 2.31023102310231e-06,
      "loss": 0.0615,
      "step": 5780
    },
    {
      "epoch": 4.7772277227722775,
      "grad_norm": 6.1482696533203125,
      "learning_rate": 2.227722772277228e-06,
      "loss": 0.0443,
      "step": 5790
    },
    {
      "epoch": 4.785478547854785,
      "grad_norm": 4.164621829986572,
      "learning_rate": 2.145214521452145e-06,
      "loss": 0.0608,
      "step": 5800
    },
    {
      "epoch": 4.793729372937293,
      "grad_norm": 3.2814671993255615,
      "learning_rate": 2.062706270627063e-06,
      "loss": 0.0494,
      "step": 5810
    },
    {
      "epoch": 4.801980198019802,
      "grad_norm": 2.9793169498443604,
      "learning_rate": 1.9801980198019803e-06,
      "loss": 0.0509,
      "step": 5820
    },
    {
      "epoch": 4.81023102310231,
      "grad_norm": 1.8670905828475952,
      "learning_rate": 1.897689768976898e-06,
      "loss": 0.0459,
      "step": 5830
    },
    {
      "epoch": 4.818481848184819,
      "grad_norm": 6.74449348449707,
      "learning_rate": 1.8151815181518153e-06,
      "loss": 0.0546,
      "step": 5840
    },
    {
      "epoch": 4.826732673267327,
      "grad_norm": 1.6954764127731323,
      "learning_rate": 1.7326732673267329e-06,
      "loss": 0.0649,
      "step": 5850
    },
    {
      "epoch": 4.834983498349835,
      "grad_norm": 3.7217841148376465,
      "learning_rate": 1.65016501650165e-06,
      "loss": 0.0618,
      "step": 5860
    },
    {
      "epoch": 4.843234323432343,
      "grad_norm": 5.863173007965088,
      "learning_rate": 1.5676567656765676e-06,
      "loss": 0.0655,
      "step": 5870
    },
    {
      "epoch": 4.851485148514851,
      "grad_norm": 2.945204734802246,
      "learning_rate": 1.4851485148514852e-06,
      "loss": 0.067,
      "step": 5880
    },
    {
      "epoch": 4.85973597359736,
      "grad_norm": 12.653159141540527,
      "learning_rate": 1.4026402640264027e-06,
      "loss": 0.0677,
      "step": 5890
    },
    {
      "epoch": 4.867986798679868,
      "grad_norm": 1.6830878257751465,
      "learning_rate": 1.3201320132013203e-06,
      "loss": 0.0448,
      "step": 5900
    },
    {
      "epoch": 4.876237623762377,
      "grad_norm": 2.505545139312744,
      "learning_rate": 1.2376237623762377e-06,
      "loss": 0.0657,
      "step": 5910
    },
    {
      "epoch": 4.884488448844884,
      "grad_norm": 3.847426176071167,
      "learning_rate": 1.155115511551155e-06,
      "loss": 0.053,
      "step": 5920
    },
    {
      "epoch": 4.8927392739273925,
      "grad_norm": 3.378716468811035,
      "learning_rate": 1.0726072607260726e-06,
      "loss": 0.0625,
      "step": 5930
    },
    {
      "epoch": 4.900990099009901,
      "grad_norm": 3.370999813079834,
      "learning_rate": 9.900990099009902e-07,
      "loss": 0.0837,
      "step": 5940
    },
    {
      "epoch": 4.909240924092409,
      "grad_norm": 3.435591459274292,
      "learning_rate": 9.075907590759076e-07,
      "loss": 0.0486,
      "step": 5950
    },
    {
      "epoch": 4.917491749174918,
      "grad_norm": 2.9726879596710205,
      "learning_rate": 8.25082508250825e-07,
      "loss": 0.0737,
      "step": 5960
    },
    {
      "epoch": 4.925742574257426,
      "grad_norm": 2.366562604904175,
      "learning_rate": 7.425742574257426e-07,
      "loss": 0.0422,
      "step": 5970
    },
    {
      "epoch": 4.933993399339934,
      "grad_norm": 4.867030143737793,
      "learning_rate": 6.600660066006602e-07,
      "loss": 0.0835,
      "step": 5980
    },
    {
      "epoch": 4.942244224422442,
      "grad_norm": 3.6714248657226562,
      "learning_rate": 5.775577557755775e-07,
      "loss": 0.0535,
      "step": 5990
    },
    {
      "epoch": 4.9504950495049505,
      "grad_norm": 4.221236705780029,
      "learning_rate": 4.950495049504951e-07,
      "loss": 0.0571,
      "step": 6000
    },
    {
      "epoch": 4.958745874587459,
      "grad_norm": 2.3105006217956543,
      "learning_rate": 4.125412541254125e-07,
      "loss": 0.0642,
      "step": 6010
    },
    {
      "epoch": 4.966996699669967,
      "grad_norm": 2.522196054458618,
      "learning_rate": 3.300330033003301e-07,
      "loss": 0.0449,
      "step": 6020
    },
    {
      "epoch": 4.975247524752476,
      "grad_norm": 3.799410343170166,
      "learning_rate": 2.4752475247524754e-07,
      "loss": 0.0369,
      "step": 6030
    },
    {
      "epoch": 4.983498349834983,
      "grad_norm": 2.378405809402466,
      "learning_rate": 1.6501650165016504e-07,
      "loss": 0.0492,
      "step": 6040
    },
    {
      "epoch": 4.991749174917492,
      "grad_norm": 0.7761074304580688,
      "learning_rate": 8.250825082508252e-08,
      "loss": 0.0468,
      "step": 6050
    },
    {
      "epoch": 5.0,
      "grad_norm": 3.407318592071533,
      "learning_rate": 0.0,
      "loss": 0.06,
      "step": 6060
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.2416648417711258,
      "eval_runtime": 39.7825,
      "eval_samples_per_second": 60.931,
      "eval_steps_per_second": 7.616,
      "step": 6060
    }
  ],
  "logging_steps": 10,
  "max_steps": 6060,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2322936878759868.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
